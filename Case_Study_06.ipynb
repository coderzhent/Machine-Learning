{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "import time\n",
    "import contextlib\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, TextVectorization\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.replace(\"Project Gutenberg\", \"\")\n",
    "    text = text.replace(\"Gutenberg\", \"\")\n",
    "\n",
    "    # Remove carriage returns\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "\n",
    "    # fix quotes\n",
    "    text = text.replace(\"“\", \"\\\"\")\n",
    "    text = text.replace(\"”\", \"\\\"\")\n",
    "\n",
    "    # Replace any capital letter at the start of a word with ^ followed by the lowercase letter\n",
    "    text = re.sub(r\"(?<![a-zA-Z])([A-Z])\", lambda match: f\"^{match.group(0).lower()}\", text)\n",
    "\n",
    "    # Replace all other capital letters with lowercase\n",
    "    text = re.sub(r\"([A-Z])\", lambda match: f\"{match.group(0).lower()}\", text)\n",
    "\n",
    "    # Remove duplicate whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\t+\", \"\\t\", text)\n",
    "\n",
    "    # Replace whitespace characters with special words\n",
    "    text = re.sub(r\"(\\t)\", r\" zztabzz \", text)\n",
    "    text = re.sub(r\"(\\n)\", r\" zznewlinezz \", text)\n",
    "    text = re.sub(r\"(\\s)\", r\" zzspacezz \", text)\n",
    "\n",
    "    # Split before and after punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, f\" {punctuation} \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(text):\n",
    "\n",
    "    # Replace special words with whitespace characters\n",
    "    text = text.replace(\"zztabzz\", \"\\t\")\n",
    "    text = text.replace(\"zznewlinezz\", \"\\n\")\n",
    "    text = text.replace(\"zzspacezz\", \" \")\n",
    "\n",
    "    # Remake capital letters at beginning of words\n",
    "    text = re.sub(r\"\\^([a-z])\", lambda match: f\"{match.group(1).upper()}\", text)\n",
    "    text = text.replace(\"^\", \"\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMyText():\n",
    "    file_name = 'austen.txt'\n",
    "    file_url = 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt'\n",
    "    local_dir = 'Module 6'  # Directory to save the file\n",
    "    local_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "\n",
    "        # Check if the file exists locally\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"File '{file_name}' found locally. Using it.\")\n",
    "        else:\n",
    "            print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
    "            # Download the file\n",
    "            downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
    "\n",
    "            # Save the downloaded file to the designated local directory\n",
    "            with open(downloaded_path, 'rb') as source_file:\n",
    "                with open(local_path, 'wb') as dest_file:\n",
    "                    dest_file.write(source_file.read())\n",
    "\n",
    "        # Read the file's contents\n",
    "        with open(local_path, 'rb') as file:\n",
    "            text = file.read().decode(encoding='utf-8')\n",
    "\n",
    "        return preprocess_text(text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomText(numbooks = 1, verbose=False):\n",
    "  download_log = io.StringIO()\n",
    "  text_random = ''\n",
    "  for b in range(numbooks):\n",
    "    foundbook = False\n",
    "    while(foundbook == False):\n",
    "      booknum = random.randint(100,60000)\n",
    "      if verbose:\n",
    "        print('Trying Book #: ',booknum)\n",
    "      if random.random() > 0.5:\n",
    "        url = 'https://www.gutenberg.org/files/' + str(booknum) + '/' + str(booknum) + '-0.txt'\n",
    "        filename_temp = str(booknum) + '-0.txt'\n",
    "      else:\n",
    "        url = 'https://www.gutenberg.org/cache/epub/' + str(booknum) + '/pg' + str(booknum) + '.txt'\n",
    "        filename_temp = 'pg' + str(booknum) + '.txt'\n",
    "      if verbose:\n",
    "        print('Trying: ', url)\n",
    "      try:\n",
    "        if verbose:\n",
    "          path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
    "        else:\n",
    "          with contextlib.redirect_stdout(download_log):\n",
    "            path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
    "        temptext = open(path_to_file_temp, 'rb').read().decode(encoding='utf-8')\n",
    "        tf.io.gfile.remove(path_to_file_temp)\n",
    "        if (temptext.find('Language: English') >= 0):\n",
    "          offset = random.randint(-20,20)\n",
    "          header = 2000\n",
    "          total_length = 200000\n",
    "          chopoffend = 10000\n",
    "          if len(temptext) > (header+total_length+offset+chopoffend):\n",
    "            foundbook = True\n",
    "            text_random += temptext[header+offset:header+total_length+offset]\n",
    "            #print(\"Yes: \" + str(booknum))\n",
    "            if verbose:\n",
    "              print('New size of dataset: ', len(text_random))\n",
    "          elif len(temptext) > (header+12000):\n",
    "            foundbook = True\n",
    "            text_random += temptext[header:-chopoffend]\n",
    "            #print(\"Yes (smaller): \" + str(booknum))\n",
    "            if verbose:\n",
    "              print('New size of dataset: ', len(text_random))\n",
    "          else:\n",
    "            if verbose:\n",
    "              print('Not long enough. Trying again...')\n",
    "            #print(\"No: \" + str(booknum) + \" too short\")\n",
    "        else:\n",
    "          if verbose:\n",
    "            print('Not English. Trying again...')\n",
    "          #print(\"No: \" + str(booknum) + \" not English\")\n",
    "        del temptext\n",
    "      except:\n",
    "        if verbose:\n",
    "          print('Not valid file. Trying again...')\n",
    "        #print(\"No: \" + str(booknum) + \" not valid\")\n",
    "        foundbook = False\n",
    "    if verbose:\n",
    "      print(\"Found \" + str(b+1) + \" books so far...\")\n",
    "  del download_log\n",
    "  #text_random = \"\".join(c for c in text_random if c in vocab)\n",
    "  #all_ids_random = ids_from_chars(tf.strings.unicode_split(text_random, 'UTF-8'))\n",
    "  #ids_dataset_random = tf.data.Dataset.from_tensor_slices(all_ids_random)\n",
    "  #sequences_random = ids_dataset_random.batch(seq_length+1, drop_remainder=True)\n",
    "  #dataset_random = sequences_random.map(split_input_target)\n",
    "  #dataset_random = (dataset_random.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "  #return dataset_random\n",
    "  return preprocess_text(text_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truns the text into a dataset\n",
    "# This function will generate our sequence pairs:\n",
    "def split_input_target(sequence):\n",
    "    input_ids = sequence[:-1]\n",
    "    target_ids = sequence[1:]\n",
    "    return input_ids, target_ids\n",
    "\n",
    "# This function will create the dataset\n",
    "def text_to_dataset(text):\n",
    "  all_ids = vectorize_layer(text)\n",
    "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "  del all_ids\n",
    "  sequences = ids_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "  del ids_dataset\n",
    "\n",
    "  # Call the function for every sequence in our list to create a new dataset\n",
    "  # of input->target pairs\n",
    "  dataset = sequences.map(split_input_target)\n",
    "  del sequences\n",
    "\n",
    "  # shuffle\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataset(dataset):\n",
    "  dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  text = ''.join([vocabulary[index] for index in ids])\n",
    "  return postprocess_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Module 6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "restart = True\n",
    "epoch_to_pickup = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'austen.txt' found locally. Using it.\n"
     ]
    }
   ],
   "source": [
    "if restart:\n",
    "  vocab_text = getMyText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 8192\n",
    "sequence_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart:\n",
    "  # Use the text vectorization layer to normalize, split, and map strings to\n",
    "  # integers. Note that the layer uses the custom standardization defined above.\n",
    "  # Set maximum_sequence length as all samples are not of the same length.\n",
    "  vectorize_layer = TextVectorization(\n",
    "      standardize='lower',\n",
    "      split='whitespace',\n",
    "      max_tokens=vocab_size,\n",
    "      output_mode='int',\n",
    "      #output_sequence_length=sequence_length\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart:\n",
    "  # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "  vectorize_layer.adapt([vocab_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart:\n",
    "  vocabulary = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart:\n",
    "  with open(path + \"vocabulary.txt\", \"w\") as file:\n",
    "    for word in vocabulary:\n",
    "        file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Vocab to a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved vocab\n",
    "if restart == False:\n",
    "  with open(path + \"vocabulary.txt\", \"r\") as file:\n",
    "      vocabulary = [word.strip() for word in file.readlines()]\n",
    "      vocabulary = vocabulary\n",
    "\n",
    "  vectorize_layer = TextVectorization(\n",
    "      vocabulary=vocabulary,\n",
    "      standardize='lower',\n",
    "      split='whitespace',\n",
    "      max_tokens=vocab_size,\n",
    "      output_mode='int',\n",
    "      #output_sequence_length=sequence_length\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('zzspacezz'), np.str_('^'), np.str_(','), np.str_('.'), np.str_('the'), np.str_('to'), np.str_('and'), np.str_('of'), np.str_('\"'), np.str_('a'), np.str_('her'), np.str_('-'), np.str_('i'), np.str_('was'), np.str_('in'), np.str_('it'), np.str_('she'), np.str_(';')]\n",
      "[np.str_('especial'), np.str_('equivocal'), np.str_('equity'), np.str_('epsom'), np.str_('epicurism'), np.str_('enumeration'), np.str_('enrich'), np.str_('enraged'), np.str_('enormity'), np.str_('engravings'), np.str_('enforcing'), np.str_('enforce'), np.str_('enemies'), np.str_('encroach'), np.str_('enclosure'), np.str_('emigrant'), np.str_('elucidation'), np.str_('eloped'), np.str_('ellison'), np.str_('eligibly')]\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[:20])\n",
    "print(vocabulary[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Vocab Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart:\n",
    "  vocab_ds = text_to_dataset(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_adjusted = vocabulary\n",
    "vocabulary_adjusted[0] = '[UNK]'\n",
    "vocabulary_adjusted[1] = ''\n",
    "\n",
    "words_from_ids = tf.keras.layers.StringLookup(vocabulary=vocabulary_adjusted, invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "tf.Tensor(\n",
      "[   3 2059    2    3   14    2    3  350    2    3   14    2    3  156\n",
      "    2    3  400    4    2  557    4    2 1075    4    2    8    2 1014\n",
      "    4    2   29    2   11    2  628    2  174    2    8    2  187    2\n",
      "  652    4    2  183    2    7    2 3698    2   98    2    9    2    6\n",
      "    2  271    2 3522    2    9    2 1804   19    2    8    2   24    2\n",
      "  813    2  899    2  633   13   75    2  345    2   16    2    6    2\n",
      "  241    2   29    2   36    2   96    2    7    2  708    2   57    2\n",
      " 4244    2   12    5    2    3   18    2   15    2    6    2 1759    2\n",
      "    9    2    6    2  122    2  643    2    9    2   11    2  107    2\n",
      "  944    4], shape=(128,), dtype=int64)\n",
      "Volume I Chapter I Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. She was the youngest of the two daughters of a most affectionate,\n",
      "tf.Tensor(\n",
      "[b'^' b'volume' b'zzspacezz' b'^' b'i' b'zzspacezz' b'^' b'chapter'\n",
      " b'zzspacezz' b'^' b'i' b'zzspacezz' b'^' b'emma' b'zzspacezz' b'^'\n",
      " b'woodhouse' b',' b'zzspacezz' b'handsome' b',' b'zzspacezz' b'clever'\n",
      " b',' b'zzspacezz' b'and' b'zzspacezz' b'rich' b',' b'zzspacezz' b'with'\n",
      " b'zzspacezz' b'a' b'zzspacezz' b'comfortable' b'zzspacezz' b'home'\n",
      " b'zzspacezz' b'and' b'zzspacezz' b'happy' b'zzspacezz' b'disposition'\n",
      " b',' b'zzspacezz' b'seemed' b'zzspacezz' b'to' b'zzspacezz' b'unite'\n",
      " b'zzspacezz' b'some' b'zzspacezz' b'of' b'zzspacezz' b'the' b'zzspacezz'\n",
      " b'best' b'zzspacezz' b'blessings' b'zzspacezz' b'of' b'zzspacezz'\n",
      " b'existence' b';' b'zzspacezz' b'and' b'zzspacezz' b'had' b'zzspacezz'\n",
      " b'lived' b'zzspacezz' b'nearly' b'zzspacezz' b'twenty' b'-' b'one'\n",
      " b'zzspacezz' b'years' b'zzspacezz' b'in' b'zzspacezz' b'the' b'zzspacezz'\n",
      " b'world' b'zzspacezz' b'with' b'zzspacezz' b'very' b'zzspacezz' b'little'\n",
      " b'zzspacezz' b'to' b'zzspacezz' b'distress' b'zzspacezz' b'or'\n",
      " b'zzspacezz' b'vex' b'zzspacezz' b'her' b'.' b'zzspacezz' b'^' b'she'\n",
      " b'zzspacezz' b'was' b'zzspacezz' b'the' b'zzspacezz' b'youngest'\n",
      " b'zzspacezz' b'of' b'zzspacezz' b'the' b'zzspacezz' b'two' b'zzspacezz'\n",
      " b'daughters' b'zzspacezz' b'of' b'zzspacezz' b'a' b'zzspacezz' b'most'\n",
      " b'zzspacezz' b'affectionate' b','], shape=(128,), dtype=string)\n",
      "Target: \n",
      "tf.Tensor(\n",
      "[2059    2    3   14    2    3  350    2    3   14    2    3  156    2\n",
      "    3  400    4    2  557    4    2 1075    4    2    8    2 1014    4\n",
      "    2   29    2   11    2  628    2  174    2    8    2  187    2  652\n",
      "    4    2  183    2    7    2 3698    2   98    2    9    2    6    2\n",
      "  271    2 3522    2    9    2 1804   19    2    8    2   24    2  813\n",
      "    2  899    2  633   13   75    2  345    2   16    2    6    2  241\n",
      "    2   29    2   36    2   96    2    7    2  708    2   57    2 4244\n",
      "    2   12    5    2    3   18    2   15    2    6    2 1759    2    9\n",
      "    2    6    2  122    2  643    2    9    2   11    2  107    2  944\n",
      "    4    2], shape=(128,), dtype=int64)\n",
      "volume I Chapter I Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. She was the youngest of the two daughters of a most affectionate, \n"
     ]
    }
   ],
   "source": [
    "if restart:\n",
    "  for input_example, target_example in vocab_ds.take(1):\n",
    "    print(\"Input: \")\n",
    "    print(input_example)\n",
    "    print(text_from_ids(input_example))\n",
    "    print(words_from_ids(input_example))\n",
    "    print(\"Target: \")\n",
    "    print(target_example)\n",
    "    print(text_from_ids(target_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart:\n",
    "  vocab_ds = setup_dataset(vocab_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First building a subclass of the tff.keras.model to have a lot more control over the intrecacies of the model. Thankfully, this is possible because tensorflow has the Functional API not just an imperative API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our custom model. Given a sequence of characters, this\n",
    "# model's job is to predict what character should come next.\n",
    "class AustenTextModel(tf.keras.Model):\n",
    "\n",
    "  # This is our class constructor method, it will be executed when\n",
    "  # we first create an instance of the class\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__()\n",
    "\n",
    "    # Our model will have three layers:\n",
    "\n",
    "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
    "    #    a vector of values suitable for a neural network\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
    "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
    "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
    "    #    then consider trying out LSTM instead (or in addition to!)\n",
    "    #self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.lstm2 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "    self.lstm3 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "    #self.lstm4 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    self.hidden1 = tf.keras.layers.Dense(embedding_dim*64, activation='relu')\n",
    "    self.hidden2 = tf.keras.layers.Dense(embedding_dim*16, activation='relu')\n",
    "    #self.hidden3 = tf.keras.layers.Dense(embedding_dim*4, activation='relu')\n",
    "\n",
    "    # 3. Our output layer that will give us a set of probabilities for each\n",
    "    #    character in our vocabulary.\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  # This function will be executed for each epoch of our training. Here\n",
    "  # we will manually feed information from one layer of our network to the\n",
    "  # next.\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "\n",
    "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
    "    #    training or predicting\n",
    "    x = self.embedding(x, training=training)\n",
    "\n",
    "    # 2. If we don't have any state in memory yet, get the initial random state\n",
    "    #    from our GRUI layer.\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "    if states is None:\n",
    "      states1 = [tf.zeros([batch_size, self.lstm1.units]), tf.zeros([batch_size, self.lstm1.units])]\n",
    "      states2 = [tf.zeros([batch_size, self.lstm2.units]), tf.zeros([batch_size, self.lstm2.units])]\n",
    "      states3 = [tf.zeros([batch_size, self.lstm3.units]), tf.zeros([batch_size, self.lstm3.units])]\n",
    "      #states4 = [tf.zeros([batch_size, self.lstm4.units]), tf.zeros([batch_size, self.lstm4.units])]\n",
    "    else:\n",
    "      states1 = states[0]\n",
    "      states2 = states[1]\n",
    "      states3 = states[2]\n",
    "      #states4 = states[3]\n",
    "    # 3. Now, feed the vectorized input along with the current state of memory\n",
    "    #    into the gru layer.\n",
    "    x, state_h_1, state_c_1 = self.lstm1(x, initial_state=states1, training=training)\n",
    "    states_out_1 = [state_h_1,state_c_1]\n",
    "\n",
    "    x, state_h_2, state_c_2 = self.lstm2(x, initial_state=states2, training=training)\n",
    "    states_out_2 = [state_h_2,state_c_2]\n",
    "\n",
    "    x, state_h_3, state_c_3 = self.lstm3(x, initial_state=states3, training=training)\n",
    "    states_out_3 = [state_h_3,state_c_3]\n",
    "\n",
    "    #x, state_h_4, state_c_4 = self.lstm4(x, initial_state=states4, training=training)\n",
    "    #states_out_4 = [state_h_4,state_c_4]\n",
    "\n",
    "    states_out = [states_out_1, states_out_2, states_out_3]#, states_out_4]\n",
    "    #states_out = [states_out_1, states_out_2]\n",
    "\n",
    "    x = self.hidden1(x,training=training)\n",
    "    x = self.hidden2(x,training=training)\n",
    "    #x = self.hidden3(x,training=training)\n",
    "    # 4. Finally, pass the results on to the dense layer\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    # 5. Return the results\n",
    "    if return_state:\n",
    "      return x, states_out\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
    "# the temperature to the distribution, and to make sure we don't get empty\n",
    "# characters in our text. Most importantly, it will keep track of our model\n",
    "# state for us.\n",
    "\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, vectorize_layer, vocabulary, temperature=1):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.vectorize_layer = vectorize_layer\n",
    "    self.vocabulary = vocabulary\n",
    "    #print(\"initialized\")\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = StringLookup(vocabulary=list(vocabulary))(['', '[UNK]'])[:, None]\n",
    "    #print(skip_ids)\n",
    "    #print(\"3\")\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(vocabulary)])\n",
    "    #print(\"4\")\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
    "    #print(\"5\")\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    #input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.vectorize_layer(inputs)\n",
    "    #print(input_ids)\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    del input_ids\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    del predicted_logits\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    #print(predicted_ids[0])\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return words_from_ids(predicted_ids), states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sample(model, vectorize_layer, vocabulary, temp, epoch, prompt):\n",
    "  # Create an instance of the character generator\n",
    "  #print(\"entered\")\n",
    "  one_step_model = OneStep(model, vectorize_layer, vocabulary, temp)\n",
    "  #print(\"rand one step\")\n",
    "  # Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
    "  # as its starting text\n",
    "  states = None\n",
    "  next_char = tf.constant([preprocess_text(prompt)])\n",
    "  result = [tf.constant([prompt])]\n",
    "\n",
    "  for n in range(200):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    #print(next_char)\n",
    "    result.append(next_char)\n",
    "    #print(result)\n",
    "\n",
    "  result = tf.strings.join(result)\n",
    "  #print(result)\n",
    "\n",
    "  # Print the results formatted.\n",
    "  #print('Temp: ' + str(temp) + '\\n')\n",
    "  print(postprocess_text(result[0].numpy().decode('utf-8')))\n",
    "  #print('\\n\\n')\n",
    "  print('Epoch: ' + str(epoch) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
    "  print('Temp: ' + str(temp) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
    "  print(postprocess_text(result[0].numpy().decode('utf-8')), file=open(path + 'tree.txt', 'a'))\n",
    "  print('\\n\\n', file=open(path + 'tree.txt', 'a'))\n",
    "  del states\n",
    "  del next_char\n",
    "  del result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart:\n",
    "  dataset = vocab_ds\n",
    "  del vocab_text\n",
    "  del vocab_ds\n",
    "else:\n",
    "  new_text = getRandomText(numbooks = 10)\n",
    "  dataset = text_to_dataset(new_text)\n",
    "  del new_text\n",
    "  dataset = setup_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our model\n",
    "#vocab_size=len(ids_from_chars.get_vocabulary())\n",
    "embedding_dim = 128\n",
    "rnn_units = 512\n",
    "\n",
    "model = AustenTextModel(vocab_size, embedding_dim, rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 8192) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Verify the output of our model is correct by running one sample through\n",
    "# This will also compile the model for us. This step will take a bit.\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"austen_text_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"austen_text_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,202,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,779,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,785,408</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │     \u001b[38;5;34m1,048,576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m,  │     \u001b[38;5;34m1,312,768\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m,  │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m,  │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))       │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8192\u001b[0m)        │     \u001b[38;5;34m4,202,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │    \u001b[38;5;34m16,779,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8192\u001b[0m)        │    \u001b[38;5;34m16,785,408\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,326,912</span> (169.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,326,912\u001b[0m (169.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,326,912</span> (169.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,326,912\u001b[0m (169.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's view the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restart == False:\n",
    "  model.load_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "File 'austen.txt' found locally. Using it.\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 3s/step - loss: 4.5620\n",
      "finished training...\n",
      "WARNING:tensorflow:From C:\\Program Files\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "dataset already deleted\n",
      "retrying epoch:  0\n",
      "epoch:  0\n",
      "File 'austen.txt' found locally. Using it.\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m966s\u001b[0m 4s/step - loss: 2.6564\n",
      "finished training...\n",
      "dataset already deleted\n",
      "retrying epoch:  0\n",
      "epoch:  0\n",
      "File 'austen.txt' found locally. Using it.\n",
      "retrying epoch:  0\n",
      "epoch:  0\n",
      "File 'austen.txt' found locally. Using it.\n",
      "\u001b[1m  2/245\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:13\u001b[0m 4s/step - loss: 2.4454"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "num_epochs_total = 5\n",
    "if restart:\n",
    "  start_epoch = 0\n",
    "else:\n",
    "  start_epoch = epoch_to_pickup\n",
    "for e in range(start_epoch, num_epochs_total):\n",
    "  success = False\n",
    "  while(success == False):\n",
    "    try:\n",
    "      print(\"epoch: \", e)\n",
    "      # if e < 50:\n",
    "      #   new_text = getRandomText(numbooks = 20)\n",
    "      # else:\n",
    "      #   new_text = sherlock_text + getRandomText(numbooks = (num_epochs_total - e)//10)\n",
    "      new_text = getMyText()\n",
    "      dataset = text_to_dataset(new_text)\n",
    "      del new_text\n",
    "      dataset = setup_dataset(dataset)\n",
    "      #opt = tf.keras.optimizers.Adam(learning_rate=0.002*(0.97**e))\n",
    "      #model.compile(optimizer=opt, loss=loss)\n",
    "      model.optimizer.learning_rate.assign(0.002*(0.99**e))\n",
    "      model.fit(dataset, epochs=1, verbose=1)\n",
    "      print(\"finished training...\")\n",
    "      del dataset\n",
    "      #print(\"saving weights...\")\n",
    "      #model.save_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")\n",
    "      #print(\"weights saved...\")\n",
    "      print(\"Dataset cleared!\")\n",
    "      for temp in [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        produce_sample(model,vectorize_layer,vocabulary, temp, e, 'Emma sat thinking about')\n",
    "      print(\"samples produced...\")\n",
    "      gc.collect()\n",
    "      print(\"garbage collected...\")\n",
    "      tf.keras.backend.clear_session()\n",
    "      print(\"session cleared (to save memory)...\")\n",
    "      #tf.config.experimental.reset_all()\n",
    "      success = True\n",
    "    except:\n",
    "      gc.collect()\n",
    "      tf.keras.backend.clear_session()\n",
    "      #tf.config.experimental.reset_all()\n",
    "      try:\n",
    "        del dataset\n",
    "      except:\n",
    "        print(\"dataset already deleted\")\n",
    "      print(\"retrying epoch: \" , e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
