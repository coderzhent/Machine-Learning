{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 6\n",
    "Pulling from examples found from 6.5 rather than the original from 6 which is plagued with issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace multiple newlines with two newlines (preserving paragraph breaks)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Lazily read the file as a CSV with one column (\"line\"), treating each line as a row.\n",
    "df = pl.scan_csv(\n",
    "    \"/content/full_books_text.txt\",\n",
    "    has_header=False,\n",
    "    new_columns=[\"line\"],\n",
    "    separator=\"\\n\",            # Use newline as the record separator\n",
    "    ignore_errors=True,        # Skip rows that cause parsing errors\n",
    "    infer_schema_length=10000, # Increase inference length for schema detection\n",
    "    schema_overrides={\"line\": pl.Utf8},\n",
    "    quote_char=None            # Disable quoting so no fields are interpreted as quoted\n",
    ")\n",
    "\n",
    "# Now, split each line into tokens (splitting on whitespace) and explode the tokens.\n",
    "tokens_df = df.with_columns(\n",
    "    pl.col(\"line\").str.split(\" \").alias(\"tokens\")\n",
    ").explode(\"tokens\")\n",
    "\n",
    "# Collect the first 100 tokens.\n",
    "first_100_tokens = tokens_df.select(\"tokens\").head(100).collect()[\"tokens\"].to_list()\n",
    "\n",
    "# Count the total number of tokens.\n",
    "total_tokens = tokens_df.select(pl.count(\"tokens\")).collect().item()\n",
    "\n",
    "# Count the unique tokens.\n",
    "unique_tokens = tokens_df.select(pl.col(\"tokens\").n_unique()).collect().item()\n",
    "\n",
    "print(\"First 100 tokens:\")\n",
    "print(\" \".join(first_100_tokens))\n",
    "print(\"\\nToken counts:\")\n",
    "print(\"Total tokens:\", total_tokens)\n",
    "print(\"Unique tokens:\", unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Collect in streaming mode to avoid memory spikes.\n",
    "limited_tokens = tokens_df_limited.select(\"tokens\").collect(streaming=True)[\"tokens\"].to_list()\n",
    "\n",
    "# In case the list has None values, filter them out.\n",
    "limited_tokens = [token for token in limited_tokens if token is not None]\n",
    "\n",
    "# Then take the first 100 tokens.\n",
    "first_100_tokens = limited_tokens[:100]\n",
    "\n",
    "print(\"First 100 tokens:\")\n",
    "print(\" \".join(first_100_tokens))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
