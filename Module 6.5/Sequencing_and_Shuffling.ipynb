{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# -----------------------\n",
        "# Data Download & Preprocessing\n",
        "# -----------------------\n",
        "print(\"[STATUS] Data Downloading & Preparation: Starting\")\n",
        "hf_files = {\n",
        "    # \"tokenized_books_1.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_books_1.npy?download=true\",\n",
        "    # \"tokenized_books_2.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_books_2.npy?download=true\",\n",
        "    # \"tokenized_books_3.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_books_3.npy?download=true\",\n",
        "    # \"tokenized_books_4.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_books_4.npy?download=true\",\n",
        "    # \"tokenized_books_5.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_books_5.npy?download=true\",\n",
        "    # \"tokenized_books_6.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_books_6.npy?download=true\",\n",
        "    # \"tokenized_books_7.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_books_7.npy?download=true\",\n",
        "    # \"tokenized_conversations_1.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_conversations_1.npy?download=true\",\n",
        "    # \"tokenized_conversations_2.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_conversations_2.npy?download=true\",\n",
        "    # \"tokenized_conversations_3.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_conversations_3.npy?download=true\",\n",
        "    # \"tokenized_conversations_4.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_conversations_4.npy?download=true\",\n",
        "    # \"tokenized_conversations_5.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_conversations_5.npy?download=true\",\n",
        "    #\"april_book.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_part_book.npy?download=true\",\n",
        "    \"bom_book.npy\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_part_bom.npy?download=true\"\n",
        "}\n",
        "\n",
        "local_files = []\n",
        "for fname, url in hf_files.items():\n",
        "    print(f\"[STATUS] Downloading {fname}\")\n",
        "    local_path = tf.keras.utils.get_file(fname, url)\n",
        "    local_files.append(local_path)\n",
        "\n",
        "all_tokens = []\n",
        "for file in local_files:\n",
        "    tokens = np.load(file, allow_pickle=False)\n",
        "    all_tokens.append(tokens)\n",
        "\n",
        "# Combine all loaded tokenized data\n",
        "combined_tokens = np.concatenate(all_tokens)\n",
        "print(f\"Total tokens across files: {len(combined_tokens)}\")\n",
        "\n",
        "# -----------------------\n",
        "# Create Sequences, Shuffle, and Write TFRecord\n",
        "# -----------------------\n",
        "\n",
        "# Define sequence length (adjust if needed)\n",
        "SEQUENCE_LENGTH = 512\n",
        "num_sequences = len(combined_tokens) // SEQUENCE_LENGTH\n",
        "print(f\"Total sequences of length {SEQUENCE_LENGTH}: {num_sequences}\")\n",
        "\n",
        "# Only keep complete sequences and reshape accordingly\n",
        "sequences = combined_tokens[:num_sequences * SEQUENCE_LENGTH].reshape(num_sequences, SEQUENCE_LENGTH)\n",
        "\n",
        "# Create a tf.data.Dataset from the sequences\n",
        "dataset = tf.data.Dataset.from_tensor_slices(sequences)\n",
        "\n",
        "# Shuffle the dataset using a buffer (parallel shuffling is built-in)\n",
        "BUFFER_SIZE = 500_000  # Adjust based on your memory/needs\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=False)\n",
        "\n",
        "# Optionally, you can batch and prefetch if further processing is required\n",
        "# dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Define a function to serialize a sequence\n",
        "def serialize_example(sequence):\n",
        "    feature = {'tokens': tf.train.Feature(int64_list=tf.train.Int64List(value=sequence))}\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "tfrecord_filename = \"tokenized_books_part4.tfrecord\"\n",
        "print(f\"[STATUS] Saving dataset as TFRecord to {tfrecord_filename}...\")\n",
        "\n",
        "# Write the shuffled sequences to TFRecord\n",
        "with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n",
        "    # Using parallel processing with tf.data might not directly speed up TFRecordWriter,\n",
        "    # but the dataset preparation (shuffling) is parallelized.\n",
        "    for sequence in dataset:\n",
        "        writer.write(serialize_example(sequence.numpy()))\n",
        "\n",
        "print(f\"✅ Saved shuffled dataset as {tfrecord_filename}\")\n",
        "\n",
        "# Conversations data is already all clean - 1_139_206 sequences for conversations 1\n",
        "# Conversations data is already all clean - 616_193 sequences for conversations 2\n",
        "\n",
        "# We end up with 1_755_399 sequences for a total of 898_764_288 million conversation tokens +165 april +712 bom\n",
        "# in other words .9 billion tokens.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM6acydctxgJ",
        "outputId": "d4dfb982-cc93-4354-a1ee-e41368c4e90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[STATUS] Data Downloading & Preparation: Starting\n",
            "[STATUS] Downloading bom_book.npy\n",
            "Downloading data from https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/tokenized_part_bom.npy?download=true\n",
            "\u001b[1m1460336/1460336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Total tokens across files: 365052\n",
            "Total sequences of length 512: 712\n",
            "[STATUS] Saving dataset as TFRecord to tokenized_books_part4.tfrecord...\n",
            "✅ Saved shuffled dataset as tokenized_books_part4.tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install tiktoken\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tiktoken\n",
        "import fasttext\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "\n",
        "# -----------------------\n",
        "# Setup: load fastText model and initialize tokenizer\n",
        "# -----------------------\n",
        "model_path = \"lid.176.ftz\"\n",
        "if not os.path.exists(model_path):\n",
        "    !wget -O {model_path} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
        "\n",
        "ft_model = fasttext.load_model(model_path)\n",
        "encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# -----------------------\n",
        "# Filtering function with newline removal\n",
        "# -----------------------\n",
        "def is_sequence_english_fasttext(sequence, encoding, threshold=0.95):\n",
        "    \"\"\"\n",
        "    Decode the token sequence using the provided encoding, remove newlines,\n",
        "    then use fastText to detect the language.\n",
        "    Returns True if the detected language is English with probability >= threshold.\n",
        "    \"\"\"\n",
        "    text = encoding.decode(sequence)\n",
        "    # Remove newlines (fastText requires a single line)\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Monkey-patch np.array temporarily to avoid fastText copy error.\n",
        "    original_np_array = np.array\n",
        "    try:\n",
        "        np.array = lambda obj, copy=False: np.asarray(obj, copy=True)\n",
        "        labels, probs = ft_model.predict(text, k=1)\n",
        "    finally:\n",
        "        np.array = original_np_array\n",
        "\n",
        "    label = labels[0]  # e.g., '__label__en'\n",
        "    prob = probs[0]\n",
        "    return label == '__label__en' and prob >= threshold\n",
        "\n",
        "def process_sequence(seq):\n",
        "    \"\"\"Return the sequence if it passes the English filter; otherwise, return None.\"\"\"\n",
        "    if is_sequence_english_fasttext(seq, encoding):\n",
        "        return seq\n",
        "    return None\n",
        "\n",
        "# -----------------------\n",
        "# Load the entire TFRecord dataset\n",
        "# -----------------------\n",
        "input_tfrecord = \"/content/tokenized_books_part4.tfrecord\"\n",
        "\n",
        "feature_description = {\n",
        "    'tokens': tf.io.FixedLenFeature([512], tf.int64),\n",
        "}\n",
        "\n",
        "def parse_function(example_proto):\n",
        "    return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(input_tfrecord)\n",
        "dataset = dataset.map(parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Comment out or remove the sharding line below to process the entire file.\n",
        "# dataset = dataset.shard(num_shards=8, index=0)\n",
        "\n",
        "sequences = []\n",
        "for record in dataset:\n",
        "    seq = record['tokens'].numpy()\n",
        "    sequences.append(seq)\n",
        "print(f\"Total sequences in the file: {len(sequences)}\")\n",
        "\n",
        "# -----------------------\n",
        "# Testing: print first 6 decoded sequences (before filtering)\n",
        "# -----------------------\n",
        "print(\"\\n--- First 6 decoded sequences (before filtering) ---\")\n",
        "for i, seq in enumerate(sequences[:6]):\n",
        "    decoded_text = encoding.decode(seq)\n",
        "    print(f\"\\nSequence {i+1}:\\n{decoded_text}\\n{'-'*40}\")\n",
        "\n",
        "# -----------------------\n",
        "# Filter sequences in parallel\n",
        "# -----------------------\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    results = list(executor.map(process_sequence, sequences))\n",
        "filtered_sequences = [seq for seq in results if seq is not None]\n",
        "print(f\"\\nTotal sequences kept after filtering: {len(filtered_sequences)}\")\n",
        "\n",
        "# -----------------------\n",
        "# Testing: print first 6 decoded sequences (after filtering)\n",
        "# -----------------------\n",
        "print(\"\\n--- First 6 decoded sequences (after filtering) ---\")\n",
        "for i, seq in enumerate(filtered_sequences[:6]):\n",
        "    decoded_text = encoding.decode(seq)\n",
        "    print(f\"\\nFiltered Sequence {i+1}:\\n{decoded_text}\\n{'-'*40}\")\n",
        "\n",
        "# -----------------------\n",
        "# Write filtered sequences to a new TFRecord file.\n",
        "# -----------------------\n",
        "output_tfrecord = \"/content/tokenized_books_part4_filtered_full.tfrecord\"\n",
        "\n",
        "def serialize_example(sequence):\n",
        "    feature = {'tokens': tf.train.Feature(int64_list=tf.train.Int64List(value=sequence))}\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "with tf.io.TFRecordWriter(output_tfrecord) as writer:\n",
        "    for seq in filtered_sequences:\n",
        "        writer.write(serialize_example(seq))\n",
        "\n",
        "print(f\"\\nFiltered sequences written to {output_tfrecord}\")\n",
        "\n",
        "\n",
        "# books one went from 1_247_288 to 1_089_431 sequences, a 13% cleanup. Probably cleaned up my tags as well.\n",
        "# books two went from 1_445_745 to 1_276_920 sequences, a 12% cleanup. Probably cleaned up my tags as well. That's ok though.\n",
        "\n",
        "# I'm left with 1_211_571_712 book tokens.\n",
        "# In other words 1.12 billion book tokens.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-HdTMLusOH8",
        "outputId": "2eab76ac-4d4a-4c94-8dcc-70d2f08c3d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313501 sha256=990b92b17b002089e7a6452c974b50e35d85b5d9147a37c69af0c3112b2a11e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n",
            "--2025-03-20 17:56:06--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.238.176.115, 18.238.176.19, 18.238.176.126, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.238.176.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 938013 (916K) [binary/octet-stream]\n",
            "Saving to: ‘lid.176.ftz’\n",
            "\n",
            "lid.176.ftz         100%[===================>] 916.03K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-03-20 17:56:06 (14.5 MB/s) - ‘lid.176.ftz’ saved [938013/938013]\n",
            "\n",
            "Total sequences in the file: 712\n",
            "\n",
            "--- First 6 decoded sequences (before filtering) ---\n",
            "\n",
            "Sequence 1:\n",
            " on the borders by the west sea. Alma 52:13 13 And thus he was endeavoring to harass the Nephites, and to draw away a part of their forces to that part of the land, while he had commanded those whom he had left to possess the cities which he had taken, that they should also harass the Nephites on the borders by the east sea, and should take possession of their lands as much as it was in their power, according to the power of their armies. Alma 52:14 14 And thus were the Nephites in those dangerous circumstances in the ending of the twenty and sixth year of the reign of the judges over the people of Nephi. Alma 52:15 15 But behold, it came to pass in the twenty and seventh year of the reign of the judges, that Teancum, by the command of Moroni--who had established armies to protect the south and the west borders of the land, and had begun his march towards the land Bountiful, that he might assist Teancum with his men in retaking the cities which they had lost-- Alma 52:16 16 And it came to pass that Teancum had received orders to make an attack upon the city of Mulek, and retake it if it were possible. Alma 52:17 17 And it came to pass that Teancum made preparations to make an attack upon the city of Mulek, and march forth with his army against the Lamanites; but he saw that it was impossible that he could overpower them while they were in their fortifications; therefore he abandoned his designs and returned again to the city Bountiful, to wait for the coming of Moroni, that he might receive strength to his army. Alma 52:18 18 And it came to pass that Moroni did arrive with his army at the land of Bountiful, in the latter end of the twenty and seventh year of the reign of the judges over the people of Nephi. Alma 52:19 19 And in the commencement of the twenty and eighth year, Moroni and Teancum and many of the chief captains held a council of war--what they should do to cause the Lamanites to come out against them to battle; or that they might by some means flatter them out of their strongholds, that they might gain advantage over them and take again the city of Mulek. Alma 52:20 20 And it came to pass they sent embassies to the army of the Lamanites, which protected the city of\n",
            "----------------------------------------\n",
            "\n",
            "Sequence 2:\n",
            " now, if the Lord has such great power, and has wrought so many miracles among the children of men, how is it that he cannot instruct me, that I should build a ship? 1 Nephi 17:52 52 And it came to pass that I, Nephi, said many things unto my brethren, insomuch that they were confounded and could not contend against me; neither durst they lay their hands upon me nor touch me with their fingers, even for the space of many days. Now they durst not do this lest they should wither before me, so powerful was the Spirit of God; and thus it had wrought upon them. 1 Nephi 17:53 53 And it came to pass that the Lord said unto me: Stretch forth thine hand again unto thy brethren, and they shall not wither before thee, but I will shock them, saith the Lord, and this will I do, that they may know that I am the Lord their God. 1 Nephi 17:54 54 And it came to pass that I stretched forth my hand unto my brethren, and they did not wither before me; but the Lord did shake them, even according to the word which he had spoken. 1 Nephi 17:55 55 And now, they said: We know of a surety that the Lord is with thee, for we know that it is the power of the Lord that has shaken us. And they fell down before me, and were about to worship me, but I would not suffer them, saying: I am thy brother, yea, even thy younger brother; wherefore, worship the Lord thy God, and honor thy father and thy mother, that thy days may be long in the land which the Lord thy God shall give thee. 1 Nephi 18 Chapter 18 1 Nephi 18:1 1 And it came to pass that they did worship the Lord, and did go forth with me; and we did work timbers of curious workmanship. And the Lord did show me from time to time after what manner I should work the timbers of the ship. 1 Nephi 18:2 2 Now I, Nephi, did not work the timbers after the manner which was learned by men, neither did I build the ship after the manner of men; but I did build it after the manner which the Lord had shown unto me; wherefore, it was not after the manner of men. 1 Nephi 18:3 3 And I, Nephi, did go into the mount\n",
            "----------------------------------------\n",
            "\n",
            "Sequence 3:\n",
            " shall be your reward in heaven; for so persecuted they the prophets who were before you. 3 Nephi 12:13 13 Verily, verily, I say unto you, I give unto you to be the salt of the earth; but if the salt shall lose its savor wherewith shall the earth be salted? The salt shall be thenceforth good for nothing, but to be cast out and to be trodden under foot of men. 3 Nephi 12:14 14 Verily, verily, I say unto you, I give unto you to be the light of this people. A city that is set on a hill cannot be hid. 3 Nephi 12:15 15 Behold, do men light a candle and put it under a bushel? Nay, but on a candlestick, and it giveth light to all that are in the house; 3 Nephi 12:16 16 Therefore let your light so shine before this people, that they may see your good works and glorify your Father who is in heaven. 3 Nephi 12:17 17 Think not that I am come to destroy the law or the prophets. I am not come to destroy but to fulfil; 3 Nephi 12:18 18 For verily I say unto you, one jot nor tittle hath not passed away from the law, but in me it hath all been fulfilled. 3 Nephi 12:19 19 And behold, I have given you the law and the commandments of my Father, that ye shall believe in me, and that ye shall repent of your sins, and come unto me with a broken heart and a contrite spirit. Behold, ye have the commandments before you, and the law is fulfilled. 3 Nephi 12:20 20 Therefore come unto me and be ye saved; for verily I say unto you, that except ye shall keep my commandments, which I have commanded you at this time, ye shall in no case enter into the kingdom of heaven. 3 Nephi 12:21 21 Ye have heard that it hath been said by them of old time, and it is also written before you, that thou shalt not kill, and whosoever shall kill shall be in danger of the judgment of God; 3 Nephi 12:22 22 But I say unto you, that whosoever is angry with his brother shall be in danger of his judgment. And whosoever shall say to his brother, Raca, shall be in danger of the council; and whosoever shall say,\n",
            "----------------------------------------\n",
            "\n",
            "Sequence 4:\n",
            " his brethren into many pieces; yea, and now behold, let us remember to keep the commandments of God, or our garments shall be rent by our brethren, and we be cast into prison, or be sold, or be slain. Alma 46:24 24 Yea, let us preserve our liberty as a remnant of Joseph; yea, let us remember the words of Jacob, before his death, for behold, he saw that a part of the remnant of the coat of Joseph was preserved and had not decayed. And he said--Even as this remnant of garment of my son hath been preserved, so shall a remnant of the seed of my son be preserved by the hand of God, and be taken unto himself, while the remainder of the seed of Joseph shall perish, even as the remnant of his garment. Alma 46:25 25 Now behold, this giveth my soul sorrow; nevertheless, my soul hath joy in my son, because of that part of his seed which shall be taken unto God. Alma 46:26 26 Now behold, this was the language of Jacob. Alma 46:27 27 And now who knoweth but what the remnant of the seed of Joseph, which shall perish as his garment, are those who have dissented from us? Yea, and even it shall be ourselves if we do not stand fast in the faith of Christ. Alma 46:28 28 And now it came to pass that when Moroni had said these words he went forth, and also sent forth in all the parts of the land where there were dissensions, and gathered together all the people who were desirous to maintain their liberty, to stand against Amalickiah and those who had dissented, who were called Amalickiahites. Alma 46:29 29 And it came to pass that when Amalickiah saw that the people of Moroni were more numerous than the Amalickiahites--and he also saw that his people were doubtful concerning the justice of the cause in which they had undertaken--therefore, fearing that he should not gain the point, he took those of his people who would and departed into the land of Nephi. Alma 46:30 30 Now Moroni thought it was not expedient that the Lamanites should have any more strength; therefore he thought to cut off the people of Amalickiah, or to take them and bring them back, and put Amalickiah to death; yea, for he knew that he would stir up the Lamanites to anger against\n",
            "----------------------------------------\n",
            "\n",
            "Sequence 5:\n",
            " feared that they would hearken to the words of Morianton and unite with his people, and thus he would obtain possession of those parts of the land, which would lay a foundation for serious consequences among the people of Nephi, yea, which consequences would lead to the overthrow of their liberty. Alma 50:33 33 Therefore Moroni sent an army, with their camp, to head the people of Morianton, to stop their flight into the land northward. Alma 50:34 34 And it came to pass that they did not head them until they had come to the borders of the land Desolation; and there they did head them, by the narrow pass which led by the sea into the land northward, yea, by the sea, on the west and on the east. Alma 50:35 35 And it came to pass that the army which was sent by Moroni, which was led by a man whose name was Teancum, did meet the people of Morianton; and so stubborn were the people of Morianton, (being inspired by his wickedness and his flattering words) that a battle commenced between them, in the which Teancum did slay Morianton and defeat his army, and took them prisoners, and returned to the camp of Moroni. And thus ended the twenty and fourth year of the reign of the judges over the people of Nephi. Alma 50:36 36 And thus were the people of Morianton brought back. And upon their covenanting to keep the peace they were restored to the land of Morianton, and a union took place between them and the people of Lehi; and they were also restored to their lands. Alma 50:37 37 And it came to pass that in the same year that the people of Nephi had peace restored unto them, that Nephihah, the second chief judge, died, having filled the judgment-seat with perfect uprightness before God. Alma 50:38 38 Nevertheless, he had refused Alma to take possession of those records and those things which were esteemed by Alma and his fathers to be most sacred; therefore Alma had conferred them upon his son, Helaman. Alma 50:39 39 Behold, it came to pass that the son of Nephihah was appointed to fill the judgment-seat, in the stead of his father; yea, he was appointed chief judge and governor over the people, with an oath and sacred ordinance to judge righteously, and to keep the peace and the freedom of the people\n",
            "----------------------------------------\n",
            "\n",
            "Sequence 6:\n",
            " had become hardened and impenitent and grossly wicked, insomuch that they did reject the word of God and all the preaching and prophesying which did come among them. Helaman 6:3 3 Nevertheless, the people of the church did have great joy because of the conversion of the Lamanites, yea, because of the church of God, which had been established among them. And they did fellowship one with another and did rejoice one with another, and did have great joy. Helaman 6:4 4 And it came to pass that many of the Lamanites did come down into the land of Zarahemla, and did declare unto the people of the Nephites the manner of their conversion, and did exhort them to faith and repentance. Helaman 6:5 5 Yea, and many did preach with exceedingly great power and authority, unto the bringing down many of them into the depths of humility, to be the humble followers of God and the Lamb. Helaman 6:6 6 And it came to pass that many of the Lamanites did go into the land northward; and also Nephi and Lehi went into the land northward, to preach unto the people. And thus ended the sixty and third year. Helaman 6:7 7 And behold, there was peace in all the land, insomuch that the Nephites did go into whatsoever part of the land they would, whether among the Nephites or the Lamanites. Helaman 6:8 8 And it came to pass that the Lamanites did also go whithersoever they would, whether it were among the Lamanites or among the Nephites; and thus they did have free intercourse one with another, to buy and to sell, and to get gain, according to their desire. Helaman 6:9 9 And it came to pass that they became exceedingly rich, both the Lamanites and the Nephites; and they did have an exceeding plenty of gold, and of silver, and of all manner of precious metals, both in the land south and in the land north. Helaman 6:10 10 Now the land south was called Lehi and the land north was called Mulek, which was after the son of Zedekiah; for the Lord did bring Mulek into the land north, and Lehi into the land south. Helaman 6:11 11 And behold, there was all manner of gold in both these lands, and of silver, and of precious ore of every kind\n",
            "----------------------------------------\n",
            "\n",
            "Total sequences kept after filtering: 704\n",
            "\n",
            "--- First 6 decoded sequences (after filtering) ---\n",
            "\n",
            "Filtered Sequence 1:\n",
            " on the borders by the west sea. Alma 52:13 13 And thus he was endeavoring to harass the Nephites, and to draw away a part of their forces to that part of the land, while he had commanded those whom he had left to possess the cities which he had taken, that they should also harass the Nephites on the borders by the east sea, and should take possession of their lands as much as it was in their power, according to the power of their armies. Alma 52:14 14 And thus were the Nephites in those dangerous circumstances in the ending of the twenty and sixth year of the reign of the judges over the people of Nephi. Alma 52:15 15 But behold, it came to pass in the twenty and seventh year of the reign of the judges, that Teancum, by the command of Moroni--who had established armies to protect the south and the west borders of the land, and had begun his march towards the land Bountiful, that he might assist Teancum with his men in retaking the cities which they had lost-- Alma 52:16 16 And it came to pass that Teancum had received orders to make an attack upon the city of Mulek, and retake it if it were possible. Alma 52:17 17 And it came to pass that Teancum made preparations to make an attack upon the city of Mulek, and march forth with his army against the Lamanites; but he saw that it was impossible that he could overpower them while they were in their fortifications; therefore he abandoned his designs and returned again to the city Bountiful, to wait for the coming of Moroni, that he might receive strength to his army. Alma 52:18 18 And it came to pass that Moroni did arrive with his army at the land of Bountiful, in the latter end of the twenty and seventh year of the reign of the judges over the people of Nephi. Alma 52:19 19 And in the commencement of the twenty and eighth year, Moroni and Teancum and many of the chief captains held a council of war--what they should do to cause the Lamanites to come out against them to battle; or that they might by some means flatter them out of their strongholds, that they might gain advantage over them and take again the city of Mulek. Alma 52:20 20 And it came to pass they sent embassies to the army of the Lamanites, which protected the city of\n",
            "----------------------------------------\n",
            "\n",
            "Filtered Sequence 2:\n",
            " now, if the Lord has such great power, and has wrought so many miracles among the children of men, how is it that he cannot instruct me, that I should build a ship? 1 Nephi 17:52 52 And it came to pass that I, Nephi, said many things unto my brethren, insomuch that they were confounded and could not contend against me; neither durst they lay their hands upon me nor touch me with their fingers, even for the space of many days. Now they durst not do this lest they should wither before me, so powerful was the Spirit of God; and thus it had wrought upon them. 1 Nephi 17:53 53 And it came to pass that the Lord said unto me: Stretch forth thine hand again unto thy brethren, and they shall not wither before thee, but I will shock them, saith the Lord, and this will I do, that they may know that I am the Lord their God. 1 Nephi 17:54 54 And it came to pass that I stretched forth my hand unto my brethren, and they did not wither before me; but the Lord did shake them, even according to the word which he had spoken. 1 Nephi 17:55 55 And now, they said: We know of a surety that the Lord is with thee, for we know that it is the power of the Lord that has shaken us. And they fell down before me, and were about to worship me, but I would not suffer them, saying: I am thy brother, yea, even thy younger brother; wherefore, worship the Lord thy God, and honor thy father and thy mother, that thy days may be long in the land which the Lord thy God shall give thee. 1 Nephi 18 Chapter 18 1 Nephi 18:1 1 And it came to pass that they did worship the Lord, and did go forth with me; and we did work timbers of curious workmanship. And the Lord did show me from time to time after what manner I should work the timbers of the ship. 1 Nephi 18:2 2 Now I, Nephi, did not work the timbers after the manner which was learned by men, neither did I build the ship after the manner of men; but I did build it after the manner which the Lord had shown unto me; wherefore, it was not after the manner of men. 1 Nephi 18:3 3 And I, Nephi, did go into the mount\n",
            "----------------------------------------\n",
            "\n",
            "Filtered Sequence 3:\n",
            " shall be your reward in heaven; for so persecuted they the prophets who were before you. 3 Nephi 12:13 13 Verily, verily, I say unto you, I give unto you to be the salt of the earth; but if the salt shall lose its savor wherewith shall the earth be salted? The salt shall be thenceforth good for nothing, but to be cast out and to be trodden under foot of men. 3 Nephi 12:14 14 Verily, verily, I say unto you, I give unto you to be the light of this people. A city that is set on a hill cannot be hid. 3 Nephi 12:15 15 Behold, do men light a candle and put it under a bushel? Nay, but on a candlestick, and it giveth light to all that are in the house; 3 Nephi 12:16 16 Therefore let your light so shine before this people, that they may see your good works and glorify your Father who is in heaven. 3 Nephi 12:17 17 Think not that I am come to destroy the law or the prophets. I am not come to destroy but to fulfil; 3 Nephi 12:18 18 For verily I say unto you, one jot nor tittle hath not passed away from the law, but in me it hath all been fulfilled. 3 Nephi 12:19 19 And behold, I have given you the law and the commandments of my Father, that ye shall believe in me, and that ye shall repent of your sins, and come unto me with a broken heart and a contrite spirit. Behold, ye have the commandments before you, and the law is fulfilled. 3 Nephi 12:20 20 Therefore come unto me and be ye saved; for verily I say unto you, that except ye shall keep my commandments, which I have commanded you at this time, ye shall in no case enter into the kingdom of heaven. 3 Nephi 12:21 21 Ye have heard that it hath been said by them of old time, and it is also written before you, that thou shalt not kill, and whosoever shall kill shall be in danger of the judgment of God; 3 Nephi 12:22 22 But I say unto you, that whosoever is angry with his brother shall be in danger of his judgment. And whosoever shall say to his brother, Raca, shall be in danger of the council; and whosoever shall say,\n",
            "----------------------------------------\n",
            "\n",
            "Filtered Sequence 4:\n",
            " his brethren into many pieces; yea, and now behold, let us remember to keep the commandments of God, or our garments shall be rent by our brethren, and we be cast into prison, or be sold, or be slain. Alma 46:24 24 Yea, let us preserve our liberty as a remnant of Joseph; yea, let us remember the words of Jacob, before his death, for behold, he saw that a part of the remnant of the coat of Joseph was preserved and had not decayed. And he said--Even as this remnant of garment of my son hath been preserved, so shall a remnant of the seed of my son be preserved by the hand of God, and be taken unto himself, while the remainder of the seed of Joseph shall perish, even as the remnant of his garment. Alma 46:25 25 Now behold, this giveth my soul sorrow; nevertheless, my soul hath joy in my son, because of that part of his seed which shall be taken unto God. Alma 46:26 26 Now behold, this was the language of Jacob. Alma 46:27 27 And now who knoweth but what the remnant of the seed of Joseph, which shall perish as his garment, are those who have dissented from us? Yea, and even it shall be ourselves if we do not stand fast in the faith of Christ. Alma 46:28 28 And now it came to pass that when Moroni had said these words he went forth, and also sent forth in all the parts of the land where there were dissensions, and gathered together all the people who were desirous to maintain their liberty, to stand against Amalickiah and those who had dissented, who were called Amalickiahites. Alma 46:29 29 And it came to pass that when Amalickiah saw that the people of Moroni were more numerous than the Amalickiahites--and he also saw that his people were doubtful concerning the justice of the cause in which they had undertaken--therefore, fearing that he should not gain the point, he took those of his people who would and departed into the land of Nephi. Alma 46:30 30 Now Moroni thought it was not expedient that the Lamanites should have any more strength; therefore he thought to cut off the people of Amalickiah, or to take them and bring them back, and put Amalickiah to death; yea, for he knew that he would stir up the Lamanites to anger against\n",
            "----------------------------------------\n",
            "\n",
            "Filtered Sequence 5:\n",
            " feared that they would hearken to the words of Morianton and unite with his people, and thus he would obtain possession of those parts of the land, which would lay a foundation for serious consequences among the people of Nephi, yea, which consequences would lead to the overthrow of their liberty. Alma 50:33 33 Therefore Moroni sent an army, with their camp, to head the people of Morianton, to stop their flight into the land northward. Alma 50:34 34 And it came to pass that they did not head them until they had come to the borders of the land Desolation; and there they did head them, by the narrow pass which led by the sea into the land northward, yea, by the sea, on the west and on the east. Alma 50:35 35 And it came to pass that the army which was sent by Moroni, which was led by a man whose name was Teancum, did meet the people of Morianton; and so stubborn were the people of Morianton, (being inspired by his wickedness and his flattering words) that a battle commenced between them, in the which Teancum did slay Morianton and defeat his army, and took them prisoners, and returned to the camp of Moroni. And thus ended the twenty and fourth year of the reign of the judges over the people of Nephi. Alma 50:36 36 And thus were the people of Morianton brought back. And upon their covenanting to keep the peace they were restored to the land of Morianton, and a union took place between them and the people of Lehi; and they were also restored to their lands. Alma 50:37 37 And it came to pass that in the same year that the people of Nephi had peace restored unto them, that Nephihah, the second chief judge, died, having filled the judgment-seat with perfect uprightness before God. Alma 50:38 38 Nevertheless, he had refused Alma to take possession of those records and those things which were esteemed by Alma and his fathers to be most sacred; therefore Alma had conferred them upon his son, Helaman. Alma 50:39 39 Behold, it came to pass that the son of Nephihah was appointed to fill the judgment-seat, in the stead of his father; yea, he was appointed chief judge and governor over the people, with an oath and sacred ordinance to judge righteously, and to keep the peace and the freedom of the people\n",
            "----------------------------------------\n",
            "\n",
            "Filtered Sequence 6:\n",
            " had become hardened and impenitent and grossly wicked, insomuch that they did reject the word of God and all the preaching and prophesying which did come among them. Helaman 6:3 3 Nevertheless, the people of the church did have great joy because of the conversion of the Lamanites, yea, because of the church of God, which had been established among them. And they did fellowship one with another and did rejoice one with another, and did have great joy. Helaman 6:4 4 And it came to pass that many of the Lamanites did come down into the land of Zarahemla, and did declare unto the people of the Nephites the manner of their conversion, and did exhort them to faith and repentance. Helaman 6:5 5 Yea, and many did preach with exceedingly great power and authority, unto the bringing down many of them into the depths of humility, to be the humble followers of God and the Lamb. Helaman 6:6 6 And it came to pass that many of the Lamanites did go into the land northward; and also Nephi and Lehi went into the land northward, to preach unto the people. And thus ended the sixty and third year. Helaman 6:7 7 And behold, there was peace in all the land, insomuch that the Nephites did go into whatsoever part of the land they would, whether among the Nephites or the Lamanites. Helaman 6:8 8 And it came to pass that the Lamanites did also go whithersoever they would, whether it were among the Lamanites or among the Nephites; and thus they did have free intercourse one with another, to buy and to sell, and to get gain, according to their desire. Helaman 6:9 9 And it came to pass that they became exceedingly rich, both the Lamanites and the Nephites; and they did have an exceeding plenty of gold, and of silver, and of all manner of precious metals, both in the land south and in the land north. Helaman 6:10 10 Now the land south was called Lehi and the land north was called Mulek, which was after the son of Zedekiah; for the Lord did bring Mulek into the land north, and Lehi into the land south. Helaman 6:11 11 And behold, there was all manner of gold in both these lands, and of silver, and of precious ore of every kind\n",
            "----------------------------------------\n",
            "\n",
            "Filtered sequences written to /content/tokenized_books_part4_filtered_full.tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Coming from a total of 1.12 billion book tokens + .9 billion conversation tokens\n",
        "# final data set size (token-wise) ~ 2 billion clean tokens divided in 4 datasets\n",
        "# 2 books datasets, 2 conversation datasets\n",
        "# these datasets contained tokenized, sequenced, and shuffled data.\n",
        "# I now check how many epochs are needed to do a full round of trianing for each dataset by using 3k steps per epoch.\n",
        "# basically figuring out how long training will take\n",
        "\n",
        "# URLs for the TFRecord files\n",
        "tfrecord_urls = {\n",
        "    \"books_part1\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_1_sequenced_shuffled_books.tfrecord?download=true\",\n",
        "    \"books_part2\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_2_sequenced_shuffled_books.tfrecord?download=true\",\n",
        "    \"conversations_part3\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_3_sequenced_shuffled_conversations.tfrecord?download=true\",\n",
        "    \"conversations_part4\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_4_sequenced_shuffled_conversations.tfrecord?download=true\",\n",
        "}\n",
        "\n",
        "# Download and save TFRecord files\n",
        "os.makedirs(\"/content/tfrecords\", exist_ok=True)\n",
        "tfrecord_files = {}\n",
        "\n",
        "for name, url in tfrecord_urls.items():\n",
        "    local_path = f\"/content/tfrecords/{name}.tfrecord\"\n",
        "    if not os.path.exists(local_path):  # Avoid re-downloading if already present\n",
        "        print(f\"Downloading {name}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=512):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "        print(f\"✅ Downloaded {name}\")\n",
        "    else:\n",
        "        print(f\"⚡ {name} already exists, skipping download.\")\n",
        "    tfrecord_files[name] = local_path\n",
        "\n",
        "# Function to count sequences in a TFRecord file\n",
        "def count_sequences(tfrecord_path):\n",
        "    count = 0\n",
        "    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "    for _ in raw_dataset:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "# Count sequences in each file\n",
        "dataset_sizes = {name: count_sequences(path) for name, path in tfrecord_files.items()}\n",
        "\n",
        "# Print the dataset sizes\n",
        "print(\"\\n📊 **Dataset Sequence Counts**\")\n",
        "for name, size in dataset_sizes.items():\n",
        "    print(f\"{name}: {size} sequences\")\n",
        "\n",
        "# Calculate recommended epochs based on 3000 steps per epoch\n",
        "# Each step processes BATCH_SIZE sequences\n",
        "BATCH_SIZE = 32  # Assuming 32 as batch size\n",
        "STEPS_PER_EPOCH = 3000  # Given in the question\n",
        "\n",
        "def recommended_epochs(dataset_size):\n",
        "    total_steps = dataset_size // BATCH_SIZE\n",
        "    return max(1, total_steps // STEPS_PER_EPOCH)  # Ensure at least 1 epoch\n",
        "\n",
        "# Compute suggested epochs for each dataset\n",
        "recommended_epochs_per_dataset = {name: recommended_epochs(size) for name, size in dataset_sizes.items()}\n",
        "\n",
        "# Print recommended epochs\n",
        "print(\"\\n📌 **Recommended Epochs for Each Dataset**\")\n",
        "for name, epochs in recommended_epochs_per_dataset.items():\n",
        "    print(f\"{name}: {epochs} epochs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RfuM2ka9IdI",
        "outputId": "68c9c040-cc82-4a4d-dda5-843ca2d03707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading books_part1...\n",
            "✅ Downloaded books_part1\n",
            "Downloading books_part2...\n",
            "✅ Downloaded books_part2\n",
            "Downloading conversations_part3...\n",
            "✅ Downloaded conversations_part3\n",
            "Downloading conversations_part4...\n",
            "✅ Downloaded conversations_part4\n",
            "\n",
            "📊 **Dataset Sequence Counts**\n",
            "books_part1: 1089431 sequences\n",
            "books_part2: 1276920 sequences\n",
            "conversations_part3: 1139206 sequences\n",
            "conversations_part4: 616193 sequences\n",
            "\n",
            "📌 **Recommended Epochs for Each Dataset**\n",
            "books_part1: 11 epochs\n",
            "books_part2: 13 epochs\n",
            "conversations_part3: 11 epochs\n",
            "conversations_part4: 6 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Suppress INFO-level messages (set to \"2\" to show warnings and errors only)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import io\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, initializers, optimizers\n",
        "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "# -----------------------\n",
        "# Utility: Download TFRecord Files from URLs\n",
        "# -----------------------\n",
        "def download_tfrecord_files(url_dict):\n",
        "    local_paths = {}\n",
        "    for fname, url in url_dict.items():\n",
        "        try:\n",
        "            path = tf.keras.utils.get_file(fname, url)\n",
        "            local_paths[fname] = path\n",
        "            tf.get_logger().info(f\"Downloaded {fname} to {path}\")\n",
        "        except Exception as e:\n",
        "            tf.get_logger().error(f\"Error downloading {fname} from {url}: {e}\")\n",
        "            raise e\n",
        "    return local_paths\n",
        "\n",
        "# -----------------------\n",
        "# URLs for the TFRecord files.\n",
        "# -----------------------\n",
        "tfrecord_urls = {\n",
        "    \"books_part1\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_1_sequenced_shuffled_books.tfrecord?download=true\",\n",
        "    \"books_part2\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_2_sequenced_shuffled_books.tfrecord?download=true\",\n",
        "    \"conversations_part3\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_3_sequenced_shuffled_conversations.tfrecord?download=true\",\n",
        "    \"conversations_part4\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_4_sequenced_shuffled_conversations.tfrecord?download=true\",\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# TFRecord Parsing Function\n",
        "# -----------------------\n",
        "def parse_tfrecord(example_proto):\n",
        "    feature_description = {'tokens': tf.io.FixedLenFeature([512], tf.int64)}\n",
        "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    tokens = parsed_example['tokens']\n",
        "    return tokens[:-1], tokens[1:]\n",
        "\n",
        "# -----------------------\n",
        "# Pre-computed Dataset Information\n",
        "# -----------------------\n",
        "\n",
        "# 📊 **Dataset Sequence Counts**\n",
        "# books_part1: 1089431 sequences\n",
        "# books_part2: 1276920 sequences\n",
        "# conversations_part3: 1139206 sequences\n",
        "# conversations_part4: 616193 sequences\n",
        "\n",
        "# 📌 **Recommended Epochs for Each Dataset**\n",
        "# books_part1: 11 epochs\n",
        "# books_part2: 13 epochs\n",
        "# conversations_part3: 11 epochs\n",
        "# conversations_part4: 6 epochs\n",
        "\n",
        "dataset_info = {\n",
        "    \"books_part1\": {\"path\": None, \"count\": 1089431, \"epochs\": 11},\n",
        "    \"books_part2\": {\"path\": None, \"count\": 1276920, \"epochs\": 13},\n",
        "    \"conversations_part3\": {\"path\": None, \"count\": 1139206, \"epochs\": 11},\n",
        "    \"conversations_part4\": {\"path\": None, \"count\": 616193, \"epochs\": 6},\n",
        "}\n",
        "\n",
        "# Download files and update dataset_info with file paths.\n",
        "local_tfrecord_paths = download_tfrecord_files(tfrecord_urls)\n",
        "for ds_name in dataset_info:\n",
        "    dataset_info[ds_name][\"path\"] = local_tfrecord_paths[ds_name]\n",
        "\n",
        "# -----------------------\n",
        "# Transformer Model Components & Helper Functions\n",
        "# -----------------------\n",
        "class RMSNorm(layers.Layer):\n",
        "    def __init__(self, epsilon=1e-8, **kwargs):\n",
        "        super(RMSNorm, self).__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name=\"gamma\",\n",
        "                                     shape=input_shape[-1:],\n",
        "                                     initializer=\"ones\",\n",
        "                                     trainable=True)\n",
        "        super(RMSNorm, self).build(input_shape)\n",
        "    def call(self, inputs):\n",
        "        gamma = tf.cast(self.gamma, inputs.dtype)\n",
        "        rms = tf.sqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + self.epsilon)\n",
        "        return inputs * gamma / rms\n",
        "\n",
        "def apply_rope(x, sin, cos):\n",
        "    sin = tf.cast(sin, x.dtype)\n",
        "    cos = tf.cast(cos, x.dtype)\n",
        "    head_dim = tf.shape(x)[-1]\n",
        "    x = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [head_dim // 2, 2]], axis=0))\n",
        "    x1, x2 = x[..., 0], x[..., 1]\n",
        "    sin_tensor = tf.expand_dims(tf.expand_dims(sin, axis=0), axis=2)\n",
        "    cos_tensor = tf.expand_dims(tf.expand_dims(cos, axis=0), axis=2)\n",
        "    x_rotated_first = x1 * cos_tensor - x2 * sin_tensor\n",
        "    x_rotated_second = x1 * sin_tensor + x2 * cos_tensor\n",
        "    x = tf.stack([x_rotated_first, x_rotated_second], axis=-1)\n",
        "    return tf.reshape(x, tf.concat([tf.shape(x)[:-2], [head_dim]], axis=0))\n",
        "\n",
        "class TiedDense(layers.Layer):\n",
        "    def __init__(self, tied_to, **kwargs):\n",
        "        super(TiedDense, self).__init__(**kwargs)\n",
        "        self.tied_to = tied_to\n",
        "    def call(self, inputs):\n",
        "        tied_embeddings = tf.cast(self.tied_to.embeddings, inputs.dtype)\n",
        "        return tf.matmul(inputs, tied_embeddings, transpose_b=True)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"tied_to\": self.tied_to.name})\n",
        "        return config\n",
        "\n",
        "class RotarySelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super(RotarySelfAttention, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        if self.head_dim * num_heads != embed_dim:\n",
        "            raise ValueError(\"embed_dim must be divisible by num_heads\")\n",
        "        self.dropout_rate = dropout_rate\n",
        "    def build(self, input_shape):\n",
        "        robust_init = initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "        self.query_dense = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.key_dense   = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.value_dense = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.out_dense   = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.dropout     = layers.Dropout(self.dropout_rate)\n",
        "        super(RotarySelfAttention, self).build(input_shape)\n",
        "    def call(self, inputs, training=False, use_causal_mask=True):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len    = tf.shape(inputs)[1]\n",
        "        query = self.query_dense(inputs)\n",
        "        key   = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = tf.reshape(query, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        key   = tf.reshape(key, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        value = tf.reshape(value, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        position = tf.cast(tf.range(seq_len), tf.float32)\n",
        "        head_dim_int = self.head_dim\n",
        "        inv_freq = 1.0 / (10000 ** (tf.cast(tf.range(0, head_dim_int, 2), tf.float32) / tf.cast(head_dim_int, tf.float32)))\n",
        "        sinusoid_inp = tf.tensordot(position, inv_freq, axes=0)\n",
        "        sin = tf.sin(sinusoid_inp)\n",
        "        cos = tf.cos(sinusoid_inp)\n",
        "        query = apply_rope(query, sin, cos)\n",
        "        key   = apply_rope(key, sin, cos)\n",
        "        query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "        key   = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "        value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "        scaling = tf.cast(self.head_dim, query.dtype) ** -0.5\n",
        "        query = query * scaling\n",
        "        attn_logits = tf.matmul(query, key, transpose_b=True)\n",
        "        if use_causal_mask:\n",
        "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len), dtype=query.dtype), -1, 0)\n",
        "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))\n",
        "            attn_logits = attn_logits * mask + tf.cast(-1e4, attn_logits.dtype) * (1 - mask)\n",
        "        attn_weights = tf.nn.softmax(attn_logits, axis=-1)\n",
        "        attn_weights = self.dropout(attn_weights, training=training)\n",
        "        attn_output = tf.matmul(attn_weights, value)\n",
        "        attn_output = tf.transpose(attn_output, perm=[0, 2, 1, 3])\n",
        "        attn_output = tf.reshape(attn_output, (batch_size, seq_len, self.embed_dim))\n",
        "        output = self.out_dense(attn_output)\n",
        "        return output\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.attention = RotarySelfAttention(embed_dim, num_heads, dropout_rate)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.norm1 = RMSNorm(epsilon=1e-8)\n",
        "        self.ffn = models.Sequential([\n",
        "            layers.Dense(ff_dim, activation=tf.nn.gelu,\n",
        "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.02)),\n",
        "            layers.Dense(embed_dim, kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
        "        ])\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        self.norm2 = RMSNorm(epsilon=1e-8)\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attention(inputs, training=training, use_causal_mask=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.norm2(out1 + ffn_output)\n",
        "\n",
        "def create_transformer_model(vocab_size, sequence_length, embed_dim, num_heads, ff_dim, num_layers, dropout_rate=0.1):\n",
        "    inputs = layers.Input(shape=(sequence_length,), dtype=tf.int32)\n",
        "    robust_init = initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "    token_embedding = layers.Embedding(input_dim=vocab_size,\n",
        "                                       output_dim=embed_dim,\n",
        "                                       embeddings_initializer=robust_init,\n",
        "                                       name=\"token_embedding\")\n",
        "    x = token_embedding(inputs)\n",
        "    for i in range(num_layers):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate, name=f\"transformer_block_{i}\")(x)\n",
        "    x = RMSNorm(epsilon=1e-8, name=\"final_rmsnorm\")(x)\n",
        "    logits = TiedDense(token_embedding, name=\"output_projection\")(x)\n",
        "    logits = layers.Lambda(lambda x: tf.cast(x, tf.float32))(logits)\n",
        "    return models.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "# -----------------------\n",
        "# Custom Perplexity Metric\n",
        "# -----------------------\n",
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='perplexity', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.ce_tracker = tf.keras.metrics.Mean(name=\"crossentropy_mean\", dtype=tf.float32)\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        ce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "        self.ce_tracker.update_state(ce, sample_weight=sample_weight)\n",
        "    def result(self):\n",
        "        avg_ce = self.ce_tracker.result()\n",
        "        return tf.exp(avg_ce)\n",
        "    def reset_state(self):\n",
        "        self.ce_tracker.reset_state()\n",
        "\n",
        "# -----------------------\n",
        "# Learning Rate and Weight Decay Schedules\n",
        "# -----------------------\n",
        "class WarmUpCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_lr, total_steps, warmup_steps, alpha=0.0):\n",
        "        super(WarmUpCosineDecay, self).__init__()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.alpha = alpha\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warmup_lr = self.initial_lr * step / tf.cast(self.warmup_steps, tf.float32)\n",
        "        cosine_steps = tf.maximum(step - tf.cast(self.warmup_steps, tf.float32), 0.0)\n",
        "        total_cosine_steps = tf.maximum(tf.cast(self.total_steps - self.warmup_steps, tf.float32), 1.0)\n",
        "        cosine_decay = 0.5 * (1 + tf.cos(np.pi * cosine_steps / total_cosine_steps))\n",
        "        decayed_lr = self.alpha * self.initial_lr + (1 - self.alpha) * self.initial_lr * cosine_decay\n",
        "        return tf.where(step < tf.cast(self.warmup_steps, tf.float32), warmup_lr, decayed_lr)\n",
        "    def get_config(self):\n",
        "        return {\"initial_lr\": self.initial_lr, \"total_steps\": self.total_steps,\n",
        "                \"warmup_steps\": self.warmup_steps, \"alpha\": self.alpha}\n",
        "\n",
        "class DynamicWeightDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, base_wd, lr_schedule):\n",
        "        super(DynamicWeightDecay, self).__init__()\n",
        "        self.base_lr = base_lr\n",
        "        self.base_wd = base_wd\n",
        "        self.lr_schedule = lr_schedule\n",
        "    def __call__(self, step):\n",
        "        current_lr = self.lr_schedule(step)\n",
        "        return self.base_wd * (current_lr / self.base_lr)\n",
        "    def get_config(self):\n",
        "        return {\"base_lr\": self.base_lr, \"base_wd\": self.base_wd}\n",
        "\n",
        "# -----------------------\n",
        "# Helper Function to Exclude Certain Parameters from Weight Decay\n",
        "# -----------------------\n",
        "def should_apply_weight_decay(var):\n",
        "    var_name = var.name.lower()\n",
        "    if \"bias\" in var_name:\n",
        "        return False\n",
        "    if \"norm\" in var_name or \"rmsnorm\" in var_name:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# -----------------------\n",
        "# Main Training Function with Interleaved Scheduling (Actual Version)\n",
        "# -----------------------\n",
        "def main():\n",
        "    # --- Build schedule using the desired interleaved order ---\n",
        "    desired_order = [\"books_part2\", \"conversations_part3\", \"books_part1\", \"conversations_part4\"]\n",
        "    # Get the maximum epoch count among the datasets in the desired order.\n",
        "    max_epochs = max(dataset_info[ds][\"epochs\"] for ds in desired_order)\n",
        "    rounds = 4\n",
        "    schedule = []\n",
        "    for r in range(rounds):\n",
        "        for e in range(1, max_epochs + 1):\n",
        "            for ds in desired_order:\n",
        "                if e <= dataset_info[ds][\"epochs\"]:\n",
        "                    schedule.append((ds, e, dataset_info[ds][\"epochs\"], r + 1))\n",
        "    total_scheduled_epochs = len(schedule)\n",
        "    print(f\"Total scheduled epochs (actual): {total_scheduled_epochs}\")\n",
        "\n",
        "    # Training parameters.\n",
        "    steps_per_epoch = 3000\n",
        "    total_steps = total_scheduled_epochs * steps_per_epoch\n",
        "    warmup_steps = int(0.1 * total_steps)\n",
        "    initial_lr = 1e-3\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # -----------------------\n",
        "    # GPU Configuration, Mixed Precision & XLA\n",
        "    # -----------------------\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f\"Number of devices (actual): {strategy.num_replicas_in_sync}\", flush=True)\n",
        "\n",
        "    # Model Hyperparameters & Tokenizer Setup.\n",
        "    SEQUENCE_LENGTH = 512\n",
        "    # Updated hyperparameters to match GPT-1 (≈117M parameters)\n",
        "    embed_dim = 768\n",
        "    num_heads = 12\n",
        "    ff_dim = 3072\n",
        "    num_layers = 12\n",
        "    dropout_rate = 0.1\n",
        "    gpt2_encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "    vocab_size = gpt2_encoding.n_vocab\n",
        "\n",
        "    # -----------------------\n",
        "    # Build the Model within the Strategy Scope.\n",
        "    # -----------------------\n",
        "    with strategy.scope():\n",
        "        model = create_transformer_model(vocab_size, SEQUENCE_LENGTH - 1,\n",
        "                                         embed_dim, num_heads, ff_dim, num_layers, dropout_rate)\n",
        "        lr_schedule = WarmUpCosineDecay(initial_lr, total_steps, warmup_steps, alpha=0.0)\n",
        "        dynamic_wd = DynamicWeightDecay(initial_lr, base_wd=1e-4, lr_schedule=lr_schedule)\n",
        "        base_optimizer = optimizers.AdamW(\n",
        "            learning_rate=lr_schedule,\n",
        "            weight_decay=0.0,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "        optimizer = LossScaleOptimizer(base_optimizer, dynamic=True)\n",
        "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # Print model summary.\n",
        "    stream = io.StringIO()\n",
        "    model.summary(print_fn=lambda x: stream.write(x + \"\\n\"))\n",
        "    print(stream.getvalue(), flush=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # Checkpointing Setup.\n",
        "    # -----------------------\n",
        "    checkpoint_dir = './checkpoints_actual'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    global_epoch = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "    ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer, epoch=global_epoch)\n",
        "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
        "    initial_global_epoch = 0\n",
        "    if ckpt_manager.latest_checkpoint:\n",
        "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "        print(f\"Restored (actual) from {ckpt_manager.latest_checkpoint}\", flush=True)\n",
        "        initial_global_epoch = int(global_epoch.numpy())\n",
        "\n",
        "    # -----------------------\n",
        "    # Metrics & Global Step.\n",
        "    # -----------------------\n",
        "    train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_perplexity_metric = Perplexity(name='train_perplexity')\n",
        "    val_loss_metric = tf.keras.metrics.Mean(name='val_loss')\n",
        "    val_perplexity_metric = Perplexity(name='val_perplexity')\n",
        "    global_step = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x_batch_train, y_batch_train, global_step):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "            scaled_loss = optimizer.get_scaled_loss(loss_value)\n",
        "        scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
        "        grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        global_step.assign_add(1)\n",
        "        current_lr = lr_schedule(tf.cast(global_step, tf.float32))\n",
        "        current_weight_decay = dynamic_wd(tf.cast(global_step, tf.float32))\n",
        "        for var in model.trainable_variables:\n",
        "            if should_apply_weight_decay(var):\n",
        "                var.assign_sub(current_lr * current_weight_decay * var)\n",
        "        return loss_value, logits\n",
        "\n",
        "    # -----------------------\n",
        "    # Lists to Store Metrics for Each Epoch\n",
        "    # -----------------------\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_perplexities = []\n",
        "    val_perplexities = []\n",
        "\n",
        "    # -----------------------\n",
        "    # Main Training Loop (Actual Version)\n",
        "    # -----------------------\n",
        "    for sched_epoch in range(initial_global_epoch, total_scheduled_epochs):\n",
        "        ds_name, epoch_in_ds, total_epochs_for_ds, current_round = schedule[sched_epoch]\n",
        "        print(f\"\\n[Actual] Round {current_round} - Training dataset '{ds_name}', epoch {epoch_in_ds}/{total_epochs_for_ds}\")\n",
        "        info = dataset_info[ds_name]\n",
        "        raw_dataset = tf.data.TFRecordDataset(info[\"path\"])\n",
        "        dataset = raw_dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        # Use 1% for validation.\n",
        "        val_size = int(0.01 * info[\"count\"])\n",
        "        val_dataset = dataset.take(val_size)\n",
        "        train_dataset = dataset.skip(val_size)\n",
        "        train_dataset = train_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
        "        train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        train_loss_metric.reset_state()\n",
        "        train_perplexity_metric.reset_state()\n",
        "        progbar = Progbar(steps_per_epoch)\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "            loss_value, logits = train_step(x_batch_train, y_batch_train, global_step)\n",
        "            train_loss_metric.update_state(loss_value)\n",
        "            train_perplexity_metric.update_state(y_batch_train, logits)\n",
        "            progbar.update(step + 1, values=[(\"loss\", train_loss_metric.result().numpy()),\n",
        "                                             (\"perplexity\", train_perplexity_metric.result().numpy())])\n",
        "\n",
        "        # Validation loop.\n",
        "        val_loss_metric.reset_state()\n",
        "        val_perplexity_metric.reset_state()\n",
        "        for x_batch_val, y_batch_val in val_dataset:\n",
        "            val_logits = model(x_batch_val, training=False)\n",
        "            val_loss = loss_fn(y_batch_val, val_logits)\n",
        "            val_loss_metric.update_state(val_loss)\n",
        "            val_perplexity_metric.update_state(y_batch_val, val_logits)\n",
        "\n",
        "        current_train_loss = train_loss_metric.result().numpy()\n",
        "        current_val_loss = val_loss_metric.result().numpy()\n",
        "        current_train_perplexity = train_perplexity_metric.result().numpy()\n",
        "        current_val_perplexity = val_perplexity_metric.result().numpy()\n",
        "\n",
        "        train_losses.append(current_train_loss)\n",
        "        val_losses.append(current_val_loss)\n",
        "        train_perplexities.append(current_train_perplexity)\n",
        "        val_perplexities.append(current_val_perplexity)\n",
        "\n",
        "        print(f\"[Actual] Epoch {sched_epoch + 1}/{total_scheduled_epochs}: Train Loss = {current_train_loss:.4f}, Train Perplexity = {current_train_perplexity:.4f}\")\n",
        "        print(f\"[Actual] Epoch {sched_epoch + 1}/{total_scheduled_epochs}: Val Loss = {current_val_loss:.4f}, Val Perplexity = {current_val_perplexity:.4f}\")\n",
        "\n",
        "        global_epoch.assign(sched_epoch + 1)\n",
        "        saved_path = ckpt_manager.save()\n",
        "        print(f\"[Actual] Checkpoint saved at: {saved_path}\", flush=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # Plotting results for all epochs.\n",
        "    # -----------------------\n",
        "    epochs_range = range(1, total_scheduled_epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs_range, train_losses, marker='o', label='Training Loss')\n",
        "    plt.plot(epochs_range, val_losses, marker='o', label='Validation Loss')\n",
        "    plt.title('Actual: Training vs. Validation Loss Over All Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs_range, train_perplexities, marker='o', label='Training Perplexity')\n",
        "    plt.plot(epochs_range, val_perplexities, marker='o', label='Validation Perplexity')\n",
        "    plt.title('Actual: Training vs. Validation Perplexity Over All Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bVjvHz9Aa7If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import io\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "import tiktoken\n",
        "\n",
        "# --- Custom Layers and Model Architecture Definitions ---\n",
        "class RMSNorm(layers.Layer):\n",
        "    def __init__(self, epsilon=1e-8, **kwargs):\n",
        "        super(RMSNorm, self).__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name=\"gamma\",\n",
        "                                     shape=input_shape[-1:],\n",
        "                                     initializer=\"ones\",\n",
        "                                     trainable=True)\n",
        "        super(RMSNorm, self).build(input_shape)\n",
        "    def call(self, inputs):\n",
        "        gamma = tf.cast(self.gamma, inputs.dtype)\n",
        "        rms = tf.sqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + self.epsilon)\n",
        "        return inputs * gamma / rms\n",
        "\n",
        "class TiedDense(layers.Layer):\n",
        "    def __init__(self, tied_to, **kwargs):\n",
        "        super(TiedDense, self).__init__(**kwargs)\n",
        "        self.tied_to = tied_to\n",
        "    def call(self, inputs):\n",
        "        tied_embeddings = tf.cast(self.tied_to.embeddings, inputs.dtype)\n",
        "        return tf.matmul(inputs, tied_embeddings, transpose_b=True)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"tied_to\": self.tied_to.name})\n",
        "        return config\n",
        "\n",
        "class RotarySelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super(RotarySelfAttention, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        if self.head_dim * num_heads != embed_dim:\n",
        "            raise ValueError(\"embed_dim must be divisible by num_heads\")\n",
        "        self.dropout_rate = dropout_rate\n",
        "    def build(self, input_shape):\n",
        "        robust_init = initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "        self.query_dense = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.key_dense   = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.value_dense = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.out_dense   = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.dropout     = layers.Dropout(self.dropout_rate)\n",
        "        super(RotarySelfAttention, self).build(input_shape)\n",
        "    def call(self, inputs, training=False, use_causal_mask=True):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len    = tf.shape(inputs)[1]\n",
        "        query = self.query_dense(inputs)\n",
        "        key   = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = tf.reshape(query, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        key   = tf.reshape(key, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        value = tf.reshape(value, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        position = tf.cast(tf.range(seq_len), tf.float32)\n",
        "        head_dim_int = self.head_dim\n",
        "        inv_freq = 1.0 / (10000 ** (tf.cast(tf.range(0, head_dim_int, 2), tf.float32) / tf.cast(head_dim_int, tf.float32)))\n",
        "        sinusoid_inp = tf.tensordot(position, inv_freq, axes=0)\n",
        "        sin = tf.sin(sinusoid_inp)\n",
        "        cos = tf.cos(sinusoid_inp)\n",
        "        # Apply RoPE\n",
        "        sin = tf.cast(sin, query.dtype)\n",
        "        cos = tf.cast(cos, query.dtype)\n",
        "        def apply_rope(x):\n",
        "            head_dim = tf.shape(x)[-1]\n",
        "            x = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [head_dim // 2, 2]], axis=0))\n",
        "            x1, x2 = x[..., 0], x[..., 1]\n",
        "            sin_tensor = tf.expand_dims(tf.expand_dims(sin, axis=0), axis=2)\n",
        "            cos_tensor = tf.expand_dims(tf.expand_dims(cos, axis=0), axis=2)\n",
        "            x_rotated_first = x1 * cos_tensor - x2 * sin_tensor\n",
        "            x_rotated_second = x1 * sin_tensor + x2 * cos_tensor\n",
        "            x = tf.stack([x_rotated_first, x_rotated_second], axis=-1)\n",
        "            return tf.reshape(x, tf.concat([tf.shape(x)[:-2], [head_dim]], axis=0))\n",
        "        query = apply_rope(query)\n",
        "        key   = apply_rope(key)\n",
        "        query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "        key   = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "        value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "        scaling = tf.cast(self.head_dim, query.dtype) ** -0.5\n",
        "        query = query * scaling\n",
        "        attn_logits = tf.matmul(query, key, transpose_b=True)\n",
        "        if use_causal_mask:\n",
        "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len), dtype=query.dtype), -1, 0)\n",
        "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))\n",
        "            attn_logits = attn_logits * mask + tf.cast(-1e4, attn_logits.dtype) * (1 - mask)\n",
        "        attn_weights = tf.nn.softmax(attn_logits, axis=-1)\n",
        "        attn_weights = self.dropout(attn_weights, training=training)\n",
        "        attn_output = tf.matmul(attn_weights, value)\n",
        "        attn_output = tf.transpose(attn_output, perm=[0, 2, 1, 3])\n",
        "        attn_output = tf.reshape(attn_output, (batch_size, seq_len, self.embed_dim))\n",
        "        output = self.out_dense(attn_output)\n",
        "        return output\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.attention = RotarySelfAttention(embed_dim, num_heads, dropout_rate)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.norm1 = RMSNorm(epsilon=1e-8)\n",
        "        self.ffn = models.Sequential([\n",
        "            layers.Dense(ff_dim, activation=tf.nn.gelu,\n",
        "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.02)),\n",
        "            layers.Dense(embed_dim, kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
        "        ])\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        self.norm2 = RMSNorm(epsilon=1e-8)\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attention(inputs, training=training, use_causal_mask=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.norm2(out1 + ffn_output)\n",
        "\n",
        "def create_transformer_model(vocab_size, sequence_length, embed_dim, num_heads, ff_dim, num_layers, dropout_rate=0.1):\n",
        "    inputs = layers.Input(shape=(sequence_length,), dtype=tf.int32)\n",
        "    robust_init = initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "    token_embedding = layers.Embedding(input_dim=vocab_size,\n",
        "                                       output_dim=embed_dim,\n",
        "                                       embeddings_initializer=robust_init,\n",
        "                                       name=\"token_embedding\")\n",
        "    x = token_embedding(inputs)\n",
        "    for i in range(num_layers):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate, name=f\"transformer_block_{i}\")(x)\n",
        "    x = RMSNorm(epsilon=1e-8, name=\"final_rmsnorm\")(x)\n",
        "    logits = TiedDense(token_embedding, name=\"output_projection\")(x)\n",
        "    logits = layers.Lambda(lambda x: tf.cast(x, tf.float32))(logits)\n",
        "    return models.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "# --- Model Hyperparameters (matching GPT-1 style, ~117M parameters) ---\n",
        "SEQUENCE_LENGTH = 512\n",
        "embed_dim = 768\n",
        "num_heads = 12\n",
        "ff_dim = 3072\n",
        "num_layers = 12\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Load GPT-2 encoding to get vocab size\n",
        "gpt2_encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = gpt2_encoding.n_vocab\n",
        "\n",
        "# Build the model\n",
        "model = create_transformer_model(vocab_size, SEQUENCE_LENGTH - 1,\n",
        "                                 embed_dim, num_heads, ff_dim, num_layers, dropout_rate)\n",
        "\n",
        "# --- Restore Weights from the Checkpoints ---\n",
        "checkpoint_dir = './checkpoints_actual'\n",
        "global_epoch = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "ckpt = tf.train.Checkpoint(model=model, epoch=global_epoch)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Checkpoint restored from:\", ckpt_manager.latest_checkpoint)\n",
        "else:\n",
        "    print(\"No checkpoint found!\")\n",
        "\n",
        "# --- Save the Entire Model to a File ---\n",
        "export_path = \"./exported_model\"\n",
        "model.save(export_path)\n",
        "print(\"Model saved to\", export_path)\n"
      ],
      "metadata": {
        "id": "cAhQfzezehFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "\n",
        "# --- Re-define Custom Layers (for model loading) ---\n",
        "class RMSNorm(tf.keras.layers.Layer):\n",
        "    def __init__(self, epsilon=1e-8, **kwargs):\n",
        "        super(RMSNorm, self).__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name=\"gamma\",\n",
        "                                     shape=input_shape[-1:],\n",
        "                                     initializer=\"ones\",\n",
        "                                     trainable=True)\n",
        "        super(RMSNorm, self).build(input_shape)\n",
        "    def call(self, inputs):\n",
        "        gamma = tf.cast(self.gamma, inputs.dtype)\n",
        "        rms = tf.sqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + self.epsilon)\n",
        "        return inputs * gamma / rms\n",
        "\n",
        "class TiedDense(tf.keras.layers.Layer):\n",
        "    def __init__(self, tied_to, **kwargs):\n",
        "        super(TiedDense, self).__init__(**kwargs)\n",
        "        self.tied_to = tied_to\n",
        "    def call(self, inputs):\n",
        "        tied_embeddings = tf.cast(self.tied_to.embeddings, inputs.dtype)\n",
        "        return tf.matmul(inputs, tied_embeddings, transpose_b=True)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"tied_to\": self.tied_to.name})\n",
        "        return config\n",
        "\n",
        "class RotarySelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super(RotarySelfAttention, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        if self.head_dim * num_heads != embed_dim:\n",
        "            raise ValueError(\"embed_dim must be divisible by num_heads\")\n",
        "        self.dropout_rate = dropout_rate\n",
        "    def build(self, input_shape):\n",
        "        robust_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "        self.query_dense = tf.keras.layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.key_dense   = tf.keras.layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.value_dense = tf.keras.layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.out_dense   = tf.keras.layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.dropout     = tf.keras.layers.Dropout(self.dropout_rate)\n",
        "        super(RotarySelfAttention, self).build(input_shape)\n",
        "    def call(self, inputs, training=False, use_causal_mask=True):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len    = tf.shape(inputs)[1]\n",
        "        query = self.query_dense(inputs)\n",
        "        key   = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = tf.reshape(query, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        key   = tf.reshape(key, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        value = tf.reshape(value, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        position = tf.cast(tf.range(seq_len), tf.float32)\n",
        "        head_dim_int = self.head_dim\n",
        "        inv_freq = 1.0 / (10000 ** (tf.cast(tf.range(0, head_dim_int, 2), tf.float32) / tf.cast(head_dim_int, tf.float32)))\n",
        "        sinusoid_inp = tf.tensordot(position, inv_freq, axes=0)\n",
        "        sin = tf.sin(sinusoid_inp)\n",
        "        cos = tf.cos(sinusoid_inp)\n",
        "        sin = tf.cast(sin, query.dtype)\n",
        "        cos = tf.cast(cos, query.dtype)\n",
        "        def apply_rope(x):\n",
        "            head_dim = tf.shape(x)[-1]\n",
        "            x = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [head_dim // 2, 2]], axis=0))\n",
        "            x1, x2 = x[..., 0], x[..., 1]\n",
        "            sin_tensor = tf.expand_dims(tf.expand_dims(sin, axis=0), axis=2)\n",
        "            cos_tensor = tf.expand_dims(tf.expand_dims(cos, axis=0), axis=2)\n",
        "            x_rotated_first = x1 * cos_tensor - x2 * sin_tensor\n",
        "            x_rotated_second = x1 * sin_tensor + x2 * cos_tensor\n",
        "            x = tf.stack([x_rotated_first, x_rotated_second], axis=-1)\n",
        "            return tf.reshape(x, tf.concat([tf.shape(x)[:-2], [head_dim]], axis=0))\n",
        "        query = apply_rope(query)\n",
        "        key   = apply_rope(key)\n",
        "        query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "        key   = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "        value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "        scaling = tf.cast(self.head_dim, query.dtype) ** -0.5\n",
        "        query = query * scaling\n",
        "        attn_logits = tf.matmul(query, key, transpose_b=True)\n",
        "        if use_causal_mask:\n",
        "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len), dtype=query.dtype), -1, 0)\n",
        "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))\n",
        "            attn_logits = attn_logits * mask + tf.cast(-1e4, attn_logits.dtype) * (1 - mask)\n",
        "        attn_weights = tf.nn.softmax(attn_logits, axis=-1)\n",
        "        attn_weights = self.dropout(attn_weights, training=training)\n",
        "        attn_output = tf.matmul(attn_weights, value)\n",
        "        attn_output = tf.transpose(attn_output, perm=[0, 2, 1, 3])\n",
        "        attn_output = tf.reshape(attn_output, (batch_size, seq_len, self.embed_dim))\n",
        "        output = self.out_dense(attn_output)\n",
        "        return output\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.attention = RotarySelfAttention(embed_dim, num_heads, dropout_rate)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.norm1 = RMSNorm(epsilon=1e-8)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=tf.nn.gelu,\n",
        "                                  kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)),\n",
        "            tf.keras.layers.Dense(embed_dim, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
        "        ])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.norm2 = RMSNorm(epsilon=1e-8)\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attention(inputs, training=training, use_causal_mask=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.norm2(out1 + ffn_output)\n",
        "\n",
        "def create_transformer_model(vocab_size, sequence_length, embed_dim, num_heads, ff_dim, num_layers, dropout_rate=0.1):\n",
        "    inputs = tf.keras.layers.Input(shape=(sequence_length,), dtype=tf.int32)\n",
        "    robust_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "    token_embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                       output_dim=embed_dim,\n",
        "                                       embeddings_initializer=robust_init,\n",
        "                                       name=\"token_embedding\")\n",
        "    x = token_embedding(inputs)\n",
        "    for i in range(num_layers):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate, name=f\"transformer_block_{i}\")(x)\n",
        "    x = RMSNorm(epsilon=1e-8, name=\"final_rmsnorm\")(x)\n",
        "    logits = TiedDense(token_embedding, name=\"output_projection\")(x)\n",
        "    logits = tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(logits)\n",
        "    return tf.keras.models.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "# --- Hyperparameters (should match those used during saving) ---\n",
        "SEQUENCE_LENGTH = 512\n",
        "embed_dim = 768\n",
        "num_heads = 12\n",
        "ff_dim = 3072\n",
        "num_layers = 12\n",
        "dropout_rate = 0.1\n",
        "gpt2_encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = gpt2_encoding.n_vocab\n",
        "\n",
        "# --- Load the Saved Model ---\n",
        "export_path = \"./exported_model\"\n",
        "model = tf.keras.models.load_model(export_path, custom_objects={\n",
        "    \"RMSNorm\": RMSNorm,\n",
        "    \"TiedDense\": TiedDense,\n",
        "    \"RotarySelfAttention\": RotarySelfAttention,\n",
        "    \"TransformerBlock\": TransformerBlock\n",
        "})\n",
        "print(\"Model loaded from\", export_path)\n",
        "\n",
        "# --- Text Generation Function ---\n",
        "def generate_text(model, prompt, num_tokens=100, temperature=1.0):\n",
        "    # Get the tokenizer encoding from tiktoken (GPT-2 encoding)\n",
        "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "    input_ids = encoding.encode(prompt)\n",
        "\n",
        "    for _ in range(num_tokens):\n",
        "        # Limit input to maximum model input length if needed\n",
        "        input_ids_cond = input_ids[-(SEQUENCE_LENGTH - 1):]\n",
        "        input_tensor = tf.convert_to_tensor([input_ids_cond])\n",
        "        logits = model(input_tensor, training=False)\n",
        "        # Get logits for the last token and apply temperature scaling\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        probs = tf.nn.softmax(logits, axis=-1).numpy()[0]\n",
        "        # Sample from the distribution\n",
        "        next_token = int(np.random.choice(len(probs), p=probs))\n",
        "        input_ids.append(next_token)\n",
        "    return encoding.decode(input_ids)\n",
        "\n",
        "# --- Generate and Print the Output ---\n",
        "prompt = \"The world seemed like such a peaceful place until the magic tree was discovered in London.\"\n",
        "generated_text = generate_text(model, prompt, num_tokens=100, temperature=1.0)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "id": "nyVAaB3Weh49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "======================="
      ],
      "metadata": {
        "id": "6Z5ot62v-J3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Suppress INFO-level messages (set to \"2\" to show warnings and errors only)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import io\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, initializers, optimizers\n",
        "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "# -----------------------\n",
        "# Utility: Download TFRecord Files from URLs\n",
        "# -----------------------\n",
        "def download_tfrecord_files(url_dict):\n",
        "    local_paths = {}\n",
        "    for fname, url in url_dict.items():\n",
        "        try:\n",
        "            path = tf.keras.utils.get_file(fname, url)\n",
        "            local_paths[fname] = path\n",
        "            tf.get_logger().info(f\"Downloaded {fname} to {path}\")\n",
        "        except Exception as e:\n",
        "            tf.get_logger().error(f\"Error downloading {fname} from {url}: {e}\")\n",
        "            raise e\n",
        "    return local_paths\n",
        "\n",
        "# -----------------------\n",
        "# URLs for the TFRecord files.\n",
        "# -----------------------\n",
        "tfrecord_urls = {\n",
        "    \"books_part1\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_1_sequenced_shuffled_books.tfrecord?download=true\",\n",
        "    # \"books_part2\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_2_sequenced_shuffled_books.tfrecord?download=true\",\n",
        "    # \"conversations_part3\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_3_sequenced_shuffled_conversations.tfrecord?download=true\",\n",
        "    # \"conversations_part4\": \"https://huggingface.co/datasets/tonadeleon/books_and_conversations/resolve/main/512_4_sequenced_shuffled_conversations.tfrecord?download=true\",\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# TFRecord Parsing Function\n",
        "# -----------------------\n",
        "def parse_tfrecord(example_proto):\n",
        "    feature_description = {'tokens': tf.io.FixedLenFeature([512], tf.int64)}\n",
        "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    tokens = parsed_example['tokens']\n",
        "    return tokens[:-1], tokens[1:]\n",
        "\n",
        "# -----------------------\n",
        "# Pre-computed Dataset Information\n",
        "# -----------------------\n",
        "\n",
        "# 📊 **Dataset Sequence Counts**\n",
        "# books_part1: 1089431 sequences\n",
        "# books_part2: 1276920 sequences\n",
        "# conversations_part3: 1139206 sequences\n",
        "# conversations_part4: 616193 sequences\n",
        "\n",
        "# 📌 **Recommended Epochs for Each Dataset**\n",
        "# books_part1: 11 epochs\n",
        "# books_part2: 13 epochs\n",
        "# conversations_part3: 11 epochs\n",
        "# conversations_part4: 6 epochs\n",
        "\n",
        "dataset_info = {\n",
        "    \"books_part1\": {\"path\": None, \"count\": 1089431, \"epochs\": 11},\n",
        "    # \"books_part2\": {\"path\": None, \"count\": 1276920, \"epochs\": 13},\n",
        "    # \"conversations_part3\": {\"path\": None, \"count\": 1139206, \"epochs\": 11},\n",
        "    # \"conversations_part4\": {\"path\": None, \"count\": 616193, \"epochs\": 6},\n",
        "}\n",
        "\n",
        "# Download files and update dataset_info with file paths.\n",
        "local_tfrecord_paths = download_tfrecord_files(tfrecord_urls)\n",
        "for ds_name in dataset_info:\n",
        "    dataset_info[ds_name][\"path\"] = local_tfrecord_paths[ds_name]\n",
        "\n",
        "# -----------------------\n",
        "# Transformer Model Components & Helper Functions\n",
        "# -----------------------\n",
        "class RMSNorm(layers.Layer):\n",
        "    def __init__(self, epsilon=1e-8, **kwargs):\n",
        "        super(RMSNorm, self).__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name=\"gamma\",\n",
        "                                     shape=input_shape[-1:],\n",
        "                                     initializer=\"ones\",\n",
        "                                     trainable=True)\n",
        "        super(RMSNorm, self).build(input_shape)\n",
        "    def call(self, inputs):\n",
        "        gamma = tf.cast(self.gamma, inputs.dtype)\n",
        "        rms = tf.sqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + self.epsilon)\n",
        "        return inputs * gamma / rms\n",
        "\n",
        "def apply_rope(x, sin, cos):\n",
        "    sin = tf.cast(sin, x.dtype)\n",
        "    cos = tf.cast(cos, x.dtype)\n",
        "    head_dim = tf.shape(x)[-1]\n",
        "    x = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [head_dim // 2, 2]], axis=0))\n",
        "    x1, x2 = x[..., 0], x[..., 1]\n",
        "    sin_tensor = tf.expand_dims(tf.expand_dims(sin, axis=0), axis=2)\n",
        "    cos_tensor = tf.expand_dims(tf.expand_dims(cos, axis=0), axis=2)\n",
        "    x_rotated_first = x1 * cos_tensor - x2 * sin_tensor\n",
        "    x_rotated_second = x1 * sin_tensor + x2 * cos_tensor\n",
        "    x = tf.stack([x_rotated_first, x_rotated_second], axis=-1)\n",
        "    return tf.reshape(x, tf.concat([tf.shape(x)[:-2], [head_dim]], axis=0))\n",
        "\n",
        "class TiedDense(layers.Layer):\n",
        "    def __init__(self, tied_to, **kwargs):\n",
        "        super(TiedDense, self).__init__(**kwargs)\n",
        "        self.tied_to = tied_to\n",
        "    def call(self, inputs):\n",
        "        tied_embeddings = tf.cast(self.tied_to.embeddings, inputs.dtype)\n",
        "        return tf.matmul(inputs, tied_embeddings, transpose_b=True)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"tied_to\": self.tied_to.name})\n",
        "        return config\n",
        "\n",
        "class RotarySelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1, **kwargs):\n",
        "        super(RotarySelfAttention, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        if self.head_dim * num_heads != embed_dim:\n",
        "            raise ValueError(\"embed_dim must be divisible by num_heads\")\n",
        "        self.dropout_rate = dropout_rate\n",
        "    def build(self, input_shape):\n",
        "        robust_init = initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "        self.query_dense = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.key_dense   = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.value_dense = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.out_dense   = layers.Dense(self.embed_dim, kernel_initializer=robust_init)\n",
        "        self.dropout     = layers.Dropout(self.dropout_rate)\n",
        "        super(RotarySelfAttention, self).build(input_shape)\n",
        "    def call(self, inputs, training=False, use_causal_mask=True):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len    = tf.shape(inputs)[1]\n",
        "        query = self.query_dense(inputs)\n",
        "        key   = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        query = tf.reshape(query, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        key   = tf.reshape(key, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        value = tf.reshape(value, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        position = tf.cast(tf.range(seq_len), tf.float32)\n",
        "        head_dim_int = self.head_dim\n",
        "        inv_freq = 1.0 / (10000 ** (tf.cast(tf.range(0, head_dim_int, 2), tf.float32) / tf.cast(head_dim_int, tf.float32)))\n",
        "        sinusoid_inp = tf.tensordot(position, inv_freq, axes=0)\n",
        "        sin = tf.sin(sinusoid_inp)\n",
        "        cos = tf.cos(sinusoid_inp)\n",
        "        query = apply_rope(query, sin, cos)\n",
        "        key   = apply_rope(key, sin, cos)\n",
        "        query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "        key   = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "        value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "        scaling = tf.cast(self.head_dim, query.dtype) ** -0.5\n",
        "        query = query * scaling\n",
        "        attn_logits = tf.matmul(query, key, transpose_b=True)\n",
        "        if use_causal_mask:\n",
        "            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len), dtype=query.dtype), -1, 0)\n",
        "            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))\n",
        "            attn_logits = attn_logits * mask + tf.cast(-1e4, attn_logits.dtype) * (1 - mask)\n",
        "        attn_weights = tf.nn.softmax(attn_logits, axis=-1)\n",
        "        attn_weights = self.dropout(attn_weights, training=training)\n",
        "        attn_output = tf.matmul(attn_weights, value)\n",
        "        attn_output = tf.transpose(attn_output, perm=[0, 2, 1, 3])\n",
        "        attn_output = tf.reshape(attn_output, (batch_size, seq_len, self.embed_dim))\n",
        "        output = self.out_dense(attn_output)\n",
        "        return output\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.attention = RotarySelfAttention(embed_dim, num_heads, dropout_rate)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.norm1 = RMSNorm(epsilon=1e-8)\n",
        "        self.ffn = models.Sequential([\n",
        "            layers.Dense(ff_dim, activation=tf.nn.gelu,\n",
        "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.02)),\n",
        "            layers.Dense(embed_dim, kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.02))\n",
        "        ])\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        self.norm2 = RMSNorm(epsilon=1e-8)\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attention(inputs, training=training, use_causal_mask=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.norm2(out1 + ffn_output)\n",
        "\n",
        "def create_transformer_model(vocab_size, sequence_length, embed_dim, num_heads, ff_dim, num_layers, dropout_rate=0.1):\n",
        "    inputs = layers.Input(shape=(sequence_length,), dtype=tf.int32)\n",
        "    robust_init = initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "    token_embedding = layers.Embedding(input_dim=vocab_size,\n",
        "                                       output_dim=embed_dim,\n",
        "                                       embeddings_initializer=robust_init,\n",
        "                                       name=\"token_embedding\")\n",
        "    x = token_embedding(inputs)\n",
        "    for i in range(num_layers):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate, name=f\"transformer_block_{i}\")(x)\n",
        "    x = RMSNorm(epsilon=1e-8, name=\"final_rmsnorm\")(x)\n",
        "    logits = TiedDense(token_embedding, name=\"output_projection\")(x)\n",
        "    logits = layers.Lambda(lambda x: tf.cast(x, tf.float32))(logits)\n",
        "    return models.Model(inputs=inputs, outputs=logits)\n",
        "\n",
        "# -----------------------\n",
        "# Custom Perplexity Metric\n",
        "# -----------------------\n",
        "class Perplexity(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='perplexity', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.ce_tracker = tf.keras.metrics.Mean(name=\"crossentropy_mean\", dtype=tf.float32)\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        ce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "        self.ce_tracker.update_state(ce, sample_weight=sample_weight)\n",
        "    def result(self):\n",
        "        avg_ce = self.ce_tracker.result()\n",
        "        return tf.exp(avg_ce)\n",
        "    def reset_state(self):\n",
        "        self.ce_tracker.reset_state()\n",
        "\n",
        "# -----------------------\n",
        "# Learning Rate and Weight Decay Schedules\n",
        "# -----------------------\n",
        "class WarmUpCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_lr, total_steps, warmup_steps, alpha=0.0):\n",
        "        super(WarmUpCosineDecay, self).__init__()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.alpha = alpha\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warmup_lr = self.initial_lr * step / tf.cast(self.warmup_steps, tf.float32)\n",
        "        cosine_steps = tf.maximum(step - tf.cast(self.warmup_steps, tf.float32), 0.0)\n",
        "        total_cosine_steps = tf.maximum(tf.cast(self.total_steps - self.warmup_steps, tf.float32), 1.0)\n",
        "        cosine_decay = 0.5 * (1 + tf.cos(np.pi * cosine_steps / total_cosine_steps))\n",
        "        decayed_lr = self.alpha * self.initial_lr + (1 - self.alpha) * self.initial_lr * cosine_decay\n",
        "        return tf.where(step < tf.cast(self.warmup_steps, tf.float32), warmup_lr, decayed_lr)\n",
        "    def get_config(self):\n",
        "        return {\"initial_lr\": self.initial_lr, \"total_steps\": self.total_steps,\n",
        "                \"warmup_steps\": self.warmup_steps, \"alpha\": self.alpha}\n",
        "\n",
        "class DynamicWeightDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, base_wd, lr_schedule):\n",
        "        super(DynamicWeightDecay, self).__init__()\n",
        "        self.base_lr = base_lr\n",
        "        self.base_wd = base_wd\n",
        "        self.lr_schedule = lr_schedule\n",
        "    def __call__(self, step):\n",
        "        current_lr = self.lr_schedule(step)\n",
        "        return self.base_wd * (current_lr / self.base_lr)\n",
        "    def get_config(self):\n",
        "        return {\"base_lr\": self.base_lr, \"base_wd\": self.base_wd}\n",
        "\n",
        "# -----------------------\n",
        "# Helper Function to Exclude Certain Parameters from Weight Decay\n",
        "# -----------------------\n",
        "def should_apply_weight_decay(var):\n",
        "    var_name = var.name.lower()\n",
        "    if \"bias\" in var_name:\n",
        "        return False\n",
        "    if \"norm\" in var_name or \"rmsnorm\" in var_name:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# -----------------------\n",
        "# Main Training Function with Interleaved Scheduling (Actual Version)\n",
        "# -----------------------\n",
        "def main():\n",
        "    # --- Build schedule using the desired interleaved order ---\n",
        "    # desired_order = [\"books_part2\", \"conversations_part3\", \"books_part1\", \"conversations_part4\"]\n",
        "    desired_order = [\"books_part1\"]\n",
        "    # Get the maximum epoch count among the datasets in the desired order.\n",
        "    # max_epochs = max(dataset_info[ds][\"epochs\"] for ds in desired_order)\n",
        "    max_epochs = 1\n",
        "    rounds = 2\n",
        "    schedule = []\n",
        "    for r in range(rounds):\n",
        "        for e in range(1, max_epochs + 1):\n",
        "            for ds in desired_order:\n",
        "                if e <= dataset_info[ds][\"epochs\"]:\n",
        "                    schedule.append((ds, e, dataset_info[ds][\"epochs\"], r + 1))\n",
        "    total_scheduled_epochs = len(schedule)\n",
        "    print(f\"Total scheduled epochs (actual): {total_scheduled_epochs}\")\n",
        "\n",
        "    # Training parameters.\n",
        "    steps_per_epoch = 3000\n",
        "    total_steps = total_scheduled_epochs * steps_per_epoch\n",
        "    warmup_steps = int(0.1 * total_steps)\n",
        "    initial_lr = 1e-3\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # -----------------------\n",
        "    # GPU Configuration, Mixed Precision & XLA\n",
        "    # -----------------------\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f\"Number of devices (actual): {strategy.num_replicas_in_sync}\", flush=True)\n",
        "\n",
        "    # Model Hyperparameters & Tokenizer Setup.\n",
        "    SEQUENCE_LENGTH = 512\n",
        "    # Updated hyperparameters to match GPT-1 (≈117M parameters)\n",
        "    embed_dim = 768\n",
        "    num_heads = 12\n",
        "    ff_dim = 3072\n",
        "    num_layers = 12\n",
        "    dropout_rate = 0.1\n",
        "    gpt2_encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "    vocab_size = gpt2_encoding.n_vocab\n",
        "\n",
        "    # -----------------------\n",
        "    # Build the Model within the Strategy Scope.\n",
        "    # -----------------------\n",
        "    with strategy.scope():\n",
        "        model = create_transformer_model(vocab_size, SEQUENCE_LENGTH - 1,\n",
        "                                         embed_dim, num_heads, ff_dim, num_layers, dropout_rate)\n",
        "        lr_schedule = WarmUpCosineDecay(initial_lr, total_steps, warmup_steps, alpha=0.0)\n",
        "        dynamic_wd = DynamicWeightDecay(initial_lr, base_wd=1e-4, lr_schedule=lr_schedule)\n",
        "        base_optimizer = optimizers.AdamW(\n",
        "            learning_rate=lr_schedule,\n",
        "            weight_decay=0.0,\n",
        "            clipnorm=1.0\n",
        "        )\n",
        "        optimizer = LossScaleOptimizer(base_optimizer, dynamic=True)\n",
        "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # Print model summary.\n",
        "    stream = io.StringIO()\n",
        "    model.summary(print_fn=lambda x: stream.write(x + \"\\n\"))\n",
        "    print(stream.getvalue(), flush=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # Checkpointing Setup.\n",
        "    # -----------------------\n",
        "    checkpoint_dir = './checkpoints_actual'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    global_epoch = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "    ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer, epoch=global_epoch)\n",
        "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
        "    initial_global_epoch = 0\n",
        "    if ckpt_manager.latest_checkpoint:\n",
        "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "        print(f\"Restored (actual) from {ckpt_manager.latest_checkpoint}\", flush=True)\n",
        "        initial_global_epoch = int(global_epoch.numpy())\n",
        "\n",
        "    # -----------------------\n",
        "    # Metrics & Global Step.\n",
        "    # -----------------------\n",
        "    train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_perplexity_metric = Perplexity(name='train_perplexity')\n",
        "    val_loss_metric = tf.keras.metrics.Mean(name='val_loss')\n",
        "    val_perplexity_metric = Perplexity(name='val_perplexity')\n",
        "    global_step = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x_batch_train, y_batch_train, global_step):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "            scaled_loss = optimizer.get_scaled_loss(loss_value)\n",
        "        scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
        "        grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        global_step.assign_add(1)\n",
        "        current_lr = lr_schedule(tf.cast(global_step, tf.float32))\n",
        "        current_weight_decay = dynamic_wd(tf.cast(global_step, tf.float32))\n",
        "        for var in model.trainable_variables:\n",
        "            if should_apply_weight_decay(var):\n",
        "                var.assign_sub(current_lr * current_weight_decay * var)\n",
        "        return loss_value, logits\n",
        "\n",
        "    # -----------------------\n",
        "    # Lists to Store Metrics for Each Epoch\n",
        "    # -----------------------\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_perplexities = []\n",
        "    val_perplexities = []\n",
        "\n",
        "    # -----------------------\n",
        "    # Main Training Loop (Actual Version)\n",
        "    # -----------------------\n",
        "    for sched_epoch in range(initial_global_epoch, total_scheduled_epochs):\n",
        "        ds_name, epoch_in_ds, total_epochs_for_ds, current_round = schedule[sched_epoch]\n",
        "        print(f\"\\n[Actual] Round {current_round} - Training dataset '{ds_name}', epoch {epoch_in_ds}/{total_epochs_for_ds}\")\n",
        "        info = dataset_info[ds_name]\n",
        "        raw_dataset = tf.data.TFRecordDataset(info[\"path\"])\n",
        "        dataset = raw_dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        # Use 1% for validation.\n",
        "        val_size = int(0.01 * info[\"count\"])\n",
        "        val_dataset = dataset.take(val_size)\n",
        "        train_dataset = dataset.skip(val_size)\n",
        "        train_dataset = train_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
        "        train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        train_loss_metric.reset_state()\n",
        "        train_perplexity_metric.reset_state()\n",
        "        progbar = Progbar(steps_per_epoch)\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "            loss_value, logits = train_step(x_batch_train, y_batch_train, global_step)\n",
        "            train_loss_metric.update_state(loss_value)\n",
        "            train_perplexity_metric.update_state(y_batch_train, logits)\n",
        "            progbar.update(step + 1, values=[(\"loss\", train_loss_metric.result().numpy()),\n",
        "                                             (\"perplexity\", train_perplexity_metric.result().numpy())])\n",
        "\n",
        "        # Validation loop.\n",
        "        val_loss_metric.reset_state()\n",
        "        val_perplexity_metric.reset_state()\n",
        "        for x_batch_val, y_batch_val in val_dataset:\n",
        "            val_logits = model(x_batch_val, training=False)\n",
        "            val_loss = loss_fn(y_batch_val, val_logits)\n",
        "            val_loss_metric.update_state(val_loss)\n",
        "            val_perplexity_metric.update_state(y_batch_val, val_logits)\n",
        "\n",
        "        current_train_loss = train_loss_metric.result().numpy()\n",
        "        current_val_loss = val_loss_metric.result().numpy()\n",
        "        current_train_perplexity = train_perplexity_metric.result().numpy()\n",
        "        current_val_perplexity = val_perplexity_metric.result().numpy()\n",
        "\n",
        "        train_losses.append(current_train_loss)\n",
        "        val_losses.append(current_val_loss)\n",
        "        train_perplexities.append(current_train_perplexity)\n",
        "        val_perplexities.append(current_val_perplexity)\n",
        "\n",
        "        print(f\"[Actual] Epoch {sched_epoch + 1}/{total_scheduled_epochs}: Train Loss = {current_train_loss:.4f}, Train Perplexity = {current_train_perplexity:.4f}\")\n",
        "        print(f\"[Actual] Epoch {sched_epoch + 1}/{total_scheduled_epochs}: Val Loss = {current_val_loss:.4f}, Val Perplexity = {current_val_perplexity:.4f}\")\n",
        "\n",
        "        global_epoch.assign(sched_epoch + 1)\n",
        "        saved_path = ckpt_manager.save()\n",
        "        print(f\"[Actual] Checkpoint saved at: {saved_path}\", flush=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # Plotting results for all epochs.\n",
        "    # -----------------------\n",
        "    epochs_range = range(1, total_scheduled_epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs_range, train_losses, marker='o', label='Training Loss')\n",
        "    plt.plot(epochs_range, val_losses, marker='o', label='Validation Loss')\n",
        "    plt.title('Actual: Training vs. Validation Loss Over All Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs_range, train_perplexities, marker='o', label='Training Perplexity')\n",
        "    plt.plot(epochs_range, val_perplexities, marker='o', label='Validation Perplexity')\n",
        "    plt.title('Actual: Training vs. Validation Perplexity Over All Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Q9ug1Ue5-LML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}